---
layout: post
title: Efficient Sampling in Probabilistic Programming

---

In probabilistic programming, is often necessary to draw samples from a space whose cardinality is much larger than the number of bits available to store it in memory. What if someone told you that no matter how large your sample space, you could draw samples in constant space and time, whilst never producing the same sample twice? This is indeed possible thanks to a beautiful theory developed by a young French mathematician named Ã‰variste Galois in the early 19th century. Today we can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!

Topics:

* Unweighted sampling without replacement in ğ’ª(1) space and time
* Efficient weighted sampling without replacement using Alias tables
* Multidimensional sampling with and without replacement
* Counting monoids and other efficient data structures
* Sample efficiency and geometric invariants

## A naÃ¯ve approach

A common problem in computer science is to generate a set of feasible solutions to a search problem, i.e., solutions meeting a set of criteria. Occasionally these solutions can be constructed directly using a clever algorithm, but in many cases, either no algorithm exists or is known for finding the solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Pursuing this idea seriously is a surprisingly delicate art.

Suppose we have a resource constrained computer and want to maximize the number of solutions found in a fixed time frame. We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(guesser: () -> List<T>, checker: (T) -> Boolean): List<T> = 
    guesser()/*All guesses must fit in memory*/.filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we filter its contents by some criteria, and only then return the results. There are a few problems here: first the search space might not fit it in memory! Second, even if it did, filtering it before returning a single answer could take more time than we have to spare. Finally, depending on how `guesser` and `checker` were implemented, it might generate repeats.

## Generators

A first improvement we could make is to incrementally guess and check using a `Sequence`. This way, we need only fit a single `T` into memory at any given point in time, and as soon as we produce a guess, it could be checked immediately:

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    stop: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < stop }

val results = search(guesser, checker) // No computation performed
//...
val output = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)` -- only when we pull from the sequence by calling `toList()` does this actually perform computation. This pattern is called a generator.

## Blind search

`Ignorance is bliss.` --Thomas Gray

In common parlance, "I don't know," might be seen as admission of ignorance or lack of understanding. On the contrary, in probabilistic programming, we call this a virtue! Serious dedication to this philosophy might even be called wisdom. The best guessers are utterly uninformed.

If all we wanted was to exhaustively search some space, we could just enumerate items in lexicographic order and be done with it. But what if the positive samples were all concentrated at the end, or in the middle, or where we least expect them? One might have to wait a very long time to catch their first lucky break. Assuming no prior knowledge about where these might be, where should we begin? Any synthetic order could be suboptimal without prior information about the domain, so the best we can do is assume positive samples are uniformly distributed across the whole space.

At first glance, uniform sampling might not seem all that useful when there are more efficient strategies available. But before we think about optimization, we must first *explore*. The key property we want from a uniform random sampler is that it explores all subspaces evenly and produces a steady stream of results -- not all clumped together in some parts and sparse in others. If there are clumpy parts with an abnormal ratio of good to bad guesses, that's not what we want.

Our sampler should produce positive and negative samples with wall-clock frequency roughly proportional to their true density across the entire sample space. To put it another way, we want the sequence of draws to be temporally invariant with respect to translation and scale: sufficiently long subsequences should all be equally representative of the full sequence. This property is known as *ergodicity*.

```
Same stream, different sampling order:

 Ergodic:
(-*--**-*-*-*-*--*-*--*--*---**-) S
 |   a   |    b      | E[a] â‰ˆ E[b]  âœ”
 |         c         | E[c] â‰ˆ E[S]  âœ”
 
 Not ergodic:
(-***-***-------*------*********) S
 |   a   |    b      | E[a] != E[b] âœ—
 |         c         | E[c] != E[S] âœ—
 
 Let Z be any statistic. For all subsequences 
 s, s' in S where |s|, |s'| in [k, |S|) and 
 sufficiently large k we want Z[s]â‰ˆZ[s'].
```

How do we ensure this property holds? We could enumerate every element, then scramble the search space using a Fisher-Yates or Knuth shuffle. Again this might work for small sets, but would require storing the entire space in memory, so we're back to square one. Instead, what if we drew a random integer between 0 and the cardinality of our sample space, then use the resulting integer to construct our guess?

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt(cardinality)) } }
    .map { i: Int -> constructData(i) }
```

There are a few subtleties here: first, we must be careful implementing `constructData()` - it should be an [injection](https://en.wikipedia.org/wiki/Injective_function) onto our datatype `<T>`. Since integers in most languages are limited to 2Â³Â², this might be an insufficient number of bits to index the full space -- we might forget to map onto some elements, or over-represent others. Second, `Random.nextInt()` will eventually draw duplicates. That might not be a big deal if the sample space is very large, but becomes problematic when sampling without replacement from small to medium sets.

Suppose we have a set too large to fit in memory but still tractable to search through. One solution might be to store a table of visited items, then filter out items previously seen:

```kotlin
fun <T> guesser(seen: MutableSet<Int> = mutableSetOf()) =
    sequence { while (true) { yield(Random.nextInt()) } }
        .filter { it !in seen } // Here we can also write .distinct()
```

This solves the issue of duplicates and runs in nearly constant time at first, but consumes an increasing amount of time and space as more samples are drawn. While there are probabilistic datastructures like Bloom filters to mitigate this growth, we are still left with the issue of rejection sampling. We want a single procedure for sampling without replacement from small, medium and large cardinality sets alike in constant time: one should be able to calculate how much wall clock time it will take to draw `K` distinct samples after drawing `J` samples for `J << K`. Let's revisit our criteria for a suitable sampler:

```
1.) Eventually visits all elements of the search space
2.) Time shift and scale invariant, i.e., ergodic
3.) Does not repeat any samples before halting
4.) Lazily evaluated, i.e., streaming algorithm
5.) Takes a constant time and space per sample
6.) Predicable runtime across long sampling runs
7.) Reproducible given the same initial seed
8.) Scales to arbitrarily-large search spaces
```

It turns out all of the above criteria are equisatisfiable. To do so, let's take a closer look at `Random`.

## Pseudorandom generators

Where does randomness come from? The best source of randomness is said to come from decaying subatomic particles. However, if we need reproducibility, we must look elsewhere. The best alternative is something called a pseudorandom number generator (PRNG), which takes an input and scrambles it up somehow to be completely unrecognizable. A good PRNG will (1) be indistinguishable from an RNG to anyone who does not know its internal state, (1) cannot be easily predicted by observing short subsequences, and (3) given the same initial state, will produce the same sequence should the need for reproducibility later arise.

Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially discrete dynamical systems known as cellular automata, Boolean dynamics, or abstract reduction systems. Some of the best PRNGs naturally have the property of also being no-replacement samplers. Below is an example of one such PRNG:

```
a  b  c  d  e     s0
1  0  0  0  0     s1
0  1  0  0  0  *  s2
0  0  1  0  0     s3
0  0  0  1  0     s4

      M        *  S
```

The variables, `a`...`e` are special bits. We will describe where they come from later. This procedure can be implemented in Kotlin as follows:

```kotlin
// Helper methods for converting to and from integers
fun List<Int>.toBitLens(): List<Int> = map { ceil(log2(it.toDouble())).toInt() }
fun List<Boolean>.toInt() = joinToString("") { if(it) "1" else "0" }.toInt(2)
fun List<Boolean>.toUInt() = joinToString("") { if(it) "1" else "0" }.toUInt(2)
fun UInt.toBitList(len: Int): List<Boolean> =
  toString(2).padStart(len, '0').map { it == '1' }

// Here we define a message passing schema
val XOR_ALGEBRA = Ring.of(
  nil = false,
  one = true,
  plus = { x, y -> x xor y },
  times = { x, y -> x and y }
)

// Any random initialization will do
private fun RandomVector(
  degree: Int,
  initialValue: UInt = 
    Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt(),
  initialState: List<Boolean> = initialValue.toBitList(degree),
) = FreeMatrix(XOR_ALGEBRA, degree, 1) { r, _ -> initialState[r] }

private fun TransitionMatrix(degree: Int, polynomial: List<Boolean>) =
  FreeMatrix(XOR_ALGEBRA, degree) { r, c -> 
    if (r == 0) polynomial[c] else c == r - 1 
  }

private fun PrimitivePolynomial(length: Int) =
    generator[length]!!.first().toString(2).map { it == '1' }

fun LFSR(
  degree: Int,
  initialVec: FreeMatrix<Boolean> = RandomVector(degree),
  polynomial: List<Boolean> = PrimitivePolynomial(degree),
  trMat: FreeMatrix<Boolean> = TransitionMatrix(degree, polynomial)
): Sequence<UInt> = sequence {
  var i = 0
  var s: FreeMatrix<Boolean> = initialVec
  do {
      s = trMat * s
      yield(s.data.toUInt())
  } while (++i < 2.0.pow(degree).toInt() - 1)
}
```

The astute reader might notice that since we are using a matrix, the time and space complexity is not constant. Due to sparsity however, we note that computing the matrix-vector product can be reduced to exactly six bitwise operations. It can be visualized as a digital circuit:

```
P = 21 = 1  0  1  0  1
         âˆ¥  âˆ¥  âˆ¥  âˆ¥  âˆ¥
         a  b  c  d  e
   â•­â”€â”€â”€â”€ âŠ• â”€â”€â”€ âŠ• â”€â”€â”€â”€â•®
   â”‚     â”‚     â”‚     â”‚
   1 --> 0  1  0  1  1 --> ğŸ—‘
 "taps" â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â•¯
 
s' =     1  0  1  0  1
s' =     0  1  0  1  0
s' =     0  0  1  0  1
...
```

By selecting the coefficients `a`..`e` carefully, we can ensure the period will be full. For example, if we use the coefficients above, i.e. `[1 0 1 0 1]`,  the state vector will not repeat itself for `31` time steps, i.e., the cardinality of the underlying set. Be assured, dear reader, this fact is no mere coincidence.

## Galois fields

*Interlude*

In the early 19th century lived a student named Evariste Galois. Of this pupil, his teachers had mixed reviews: "intelligence, marked progress, not enough method", "withdrawn and original". He twice tried and twice failed to gain entrance to l'Ecole Polytechnique, then the premier math institute in France. His papers were desk rejected by Cauchy, deemed "incomprehensible... neither sufficiently clear nor sufficiently developed to allow us to judge its rigor," by Poisson. But then something remarkable happened. For reasons yet unclear -- perhaps for his political beliefs or perhaps unrequited love -- he was engaged in a duel. As legend goes, young Galois, sensing his demise stayed up the night before this fateful event imploring his friends to make sure his life's work did not perish with him. He writes, "I am fighting against my will, having exhausted all possible means of reconciliation... forgive those who kill me for they are of good faith" ... "[enclosed] I have done several new things in analysis... Ask Jacobi or Gauss to give their opinion not on the truth but on the importance of the theorems." On the dawn of May 30th, 1832 the duelists stood across a misty field. A shot rang out, claiming Galois' life. He was just shy of 21 years old.

A Galois field is an algebraic structure `<F, +, *>` with the following properties:

- **Finite**: It has a finite number of elements.
- **Closure**: Any operation on two members of the Galois field produces another member of the Galois field.
- **Additive and multiplicative associativity**: Parentheses around additive and multiplicative sequences can be added wheresoever one pleases, i.e., `a*(b*c) = (a*b)*c`, `a+(b+c) = (a+b)+c`.
- **Additive and multiplicative commutativity**: Both addition and multiplication are symmetric in their parameter order, i.e., `a*b=b*a`, `a+b=b+a`.
- **Multiplicative Distributivity**: Multiplication distributes over addition, i.e., `a*(b+c)=a*b+a*c`.
- **Additive and multiplicative identity**: There are two distinct elements `0` and `1`, which added or multiplied (respectively) with any other element, will not disturb it, i.e., `0+a=a`, `1*a=a`, `1â‰ 0`.
- **Additive and multiplicative inverse**: For each `a` there is an element `-a` such that `a+(-a)=0`. Similarly, for each nonzero `a` there is an element `1/a` such that `a*(1/a)=1`.

These structures have lots of applications in number theory, computer science, cryptography, and as it turns out, probabilistic programming. We can describe Galois fields of degree `2â¿` using a special kind of function that maps the field back onto itself called a feedback polynomial. A subset of these, called primitive polynomials, have the envious property of being minimal length and maximal periodicity. These two properties make it highly desirable: a primitive polynomial on `GF(2â¿)` can generate the entire field using only ~ğ’ª(log(n)) bits.

This means the primitive polynomial is a space-filling curve, i.e., a curve which visits all `2â¿-1` elements of the set exactly once before repeating. Furthermore, it traverses the space in pseudorandom order: unlike simple enumerative procedures like Hilbert Curves or Gray Codes, the PP takes a nontrivial trajectory through state space making it difficult, though not impossible, to predict by observing inputs and outputs. More inversion-resistant alternatives exist if secrecy is important.

Among it many useful properties, we can use the primitive polynomial to construct a finite state machine or linear finite state register (LFSR) whose intermediate states are all nearly indistinguishable from a true RNG on many summary statistics. This makes them useful proxies for simulating stochasticity on a deterministic machine. Their maximal period allows us to generate the entire set by iterating its primitive polynomial, which enables them to be used for unbiased sampling without replacement.

How do we find primitive polynomials? We could simply enumerate all degree-S polynomials up to `logâ‚‚(|S|)` and check whether they are full period. It so happens we have already seen a guess-and-check algorithm that can generate examples in random order. This search procedure works for low-order polynomials but grows intractable with increasing degree.

The beauty of primitive polynomials over GF(2) is that they can be composed using other primitive polynomials. This fact allows us to write a decision procedure which eliminates certain guesses using a static property of the polynomial, rather than measuring its periodicity by simulating the dynamics, which might take until the heat death of the universe to exhaustively check.

A degree-r polynomial is primitive iff its period is `2Ê³ - 1`...

It turns out we can use the same matrix multiplication procedure we used to simulate the dynamics, to find solutions for the statics.

Below is an excerpt from Galois' last memoirs, written during his late teenage years, shortly before his untimely death at age 20.

<div class="quote"><blockquote><b>PrÃ©face pour â€œDeux MÃ©moires dâ€™Analyse Pureâ€</b> â€“ Ã‰variste Galois, 1811-1832 aprÃ¨s J.-C.<br>

Les longs calculs algÃ©briques ont dâ€™abord Ã©tÃ© peu nÃ©cessaires au progrÃ¨s des mathÃ©matiques, les thÃ©orÃ¨mes fort simples gagnaient Ã  peine Ã  Ãªtre traduits dans la langue de lâ€™analyse. Ce nâ€™est guÃ¨re que depuis Euler que cette langue plus brÃ¨ve est devenue indispensable Ã  la nouvelle extension que ce grand gÃ©omÃ¨tre a donnÃ© Ã  la science. Depuis Euler, les calculs sont devenus de plus en plus nÃ©cessaires, mais de plus en plus difficiles... lâ€™algorithme avait atteint un degrÃ© de complication tel que tout progrÃ¨s Ã©tait devenu impossible par ce moyen, sans lâ€™Ã©lÃ©gance que les gÃ©omÃ¨tres modernes ont su imprimer Ã  leurs recherches et au moyen de laquelle l'esprit saisit promptement et dâ€™un seul coup un grand nombre dâ€™opÃ©rations.

[...]

Il est Ã©vident que lâ€™Ã©lÃ©gance, si vantÃ©e et Ã  si juste titre, nâ€™a pas dâ€™autre but.

[...]

Sauter Ã  pieds joints sur ces calculs; grouper les opÃ©rations, les classer suivant leurs difficultÃ©s et non suivant leurs formes; telle est, suivant moi, la mission des gÃ©omÃ¨tres futurs; telle est la voie oÃ¹ je suis entrÃ© dans cet ouvrage.
</blockquote></div>

<div class="quote"><blockquote><b>Preface to â€œTwo Memoirs on Pure Analysisâ€</b> â€“ Evariste Galois, A.D. 1811-1832<br>

Long algebraic calculations were at first hardly necessary for progress in mathematics; the very simple theorems benefited little from being translated into the language of analysis. It is only since Euler this concision has become indispensable to continuing the work this great geometer has given to science. Since Euler, calculation has become more and more necessary and difficult... the algorithms so complicated that progress has become impossible without the elegance that modern geometers have brought to bear on their research, and by which means the mind can promptly and with a single glance grasp a large number of operations.

[...]

It is clear that elegance, so admired and so justly named, has no other purpose.

[...]

Jump with both feet into the calculations! Group the operations, classify them according to their difficulties and not according to their appearances. This, I believe, is the mission of future geometers. This is the road on which I am embarking in this work.
</blockquote></div>

## Multidimensional sampling

So we can sample random elements of a single set, but how do we generalize this to higher dimensional sets? This would be useful if we wanted to sample without replacement from a fixed-length sequence of draws, such as a sequence of words in natural language or a hand of cards from a deck. Faithfully reconstructing distributions of this kind requires modeling the full joint distribution. Let's revisit the enumerative guesser we saw earlier. We want to generalize this sampling procedure to higher dimensional distributions.

Typically, given a sequence of words `T=tâ‚, tâ‚‚, ..., tâ‚™`, one would assume the probability `P(T) = P(tâ‚) P(tâ‚‚|tâ‚) P(tâ‚ƒ|tâ‚,tâ‚‚) P(tâ‚„|tâ‚,tâ‚‚,tâ‚ƒ) ... P(tâ‚™|tâ‚, tâ‚‚, ..., tâ‚™â‚‹â‚)` factorizes into the product of conditionally-independent events. Called the Markov assumption, this admits a much more tractable solution in the form `P(T) â‰ˆ P(tâ‚) P(tâ‚‚|tâ‚) P(tâ‚ƒ|tâ‚‚) P(tâ‚„|tâ‚ƒ) ... P(tâ‚™|tâ‚™â‚‹â‚)`, but can be a gross simplification. For example, the probability of a word in natural language depends not only on the preceding word or words, but also on those that follow.

What if we really did want to model the full joint distribution `P(tâ‚,tâ‚‚,tâ‚ƒ, ..., tâ‚™)`? This would require instantiating a multidimensional array storing a distribution over `Î£â¿`-- often considered a nonstarter due to its exponential size. As we will see later, we could represent this distribution using a more efficient data structure. But perhaps surprisingly, even if we cannot instantiate the joint distribution, we can still sample uniformly without replacement from the event space for *staggeringly large* values of `n`.

Let's suppose we just want to sample length-4 strings from the binary alphabet, `Î£ = {0, 1}`. We could call `sample(setOf("0", "1"), 4)` on the following procedure:

```kotlin
fun <T> sample(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
  sample(List(dimension) { base })

fun <T> sample(
  dimensions: List<Set<T>>,
  cardinalities: List<Int> = dimensions.map { it.size },
  asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> =
  draw(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun draw(i: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
  if (i.isEmpty()) sequenceOf(l)
  else (0 until i[0]).asSequence()
    .shuffled().flatMap { draw(i.drop(1), l + it) }
```

The problem here is that the bit patterns are fixed depth-first. Notice that if we draw an unlucky prefix, it might take `draw` a very long time to back out of that assignment:

```
L = { 0(0|1)* }

   â•­â”€[Bad initialization!]
  [1]00010 âˆ‰ L
  [1]00011 âˆ‰ L
  [1]00001 âˆ‰ L
  [1]00000 âˆ‰ L
  [1]00101 âˆ‰ L
  ...
```

Instead, we would prefer if each substring is equiprobable (modulo replacement) on every draw. Remember the ergodicity criteria from earlier: we would like to generate a sequence of strings where each subsequence of sufficient length is statistically uniform. How do we achieve this? Let's assume the alphabet is a power of two. Other bases are also possible, but powers of two are convenient for a variety of reasons.

```
Î£' = { A, B, C, D, E, F, G, H, I, J, K, L, M, N, O } 
  C1     C2     C3     C4
  ||     ||     ||     ||
[1110] [1011] [1010] [0010]
  N      K      J      B
```

So in order to generate quadruplets from the alphabet `Î£'`, we would need a bit string of length 16. Given a primitive polynomial over a bitvector of length 16, we could sample without replacement from the space `Î£â´`, since both `Î£â´` and `GF(2^16)` have exactly the same cardinality. How convenient!

So we can generate Boolean vectors. How do we map this back into vector spaces, whose cardinality is not necessarily an even power of two? A first thought might be to use the leading bits, but that would break the injection, since the polynomial is only guaranteed to be aperiodic up to its full degree. Instead, we will encode the intermediate state into variable-length chunks:

```kotlin
fun List<Int>.toBitLens() = map { ceil(log2(it.toDouble())).toInt() }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

val dimensions: List<Set<Char>> = 
    listOf('a'..'g', 'h'..'j', 'k'..'q', 'r'..'t').map { it.toSet() }
val bitLens = dimensions.map { it.size }.toBitLens()
val state = "1010101100".map { it == '1' }.toIndexes(bitLens)
// State is now [4, 1, 3, 0]
```

These chunks will represent individual dimensions of the hyperdistribution. If each slot is drawn from the same sample space we simply assign equal-length bitvectors, requiring `s = d * âŒˆlogâ‚‚(c)âŒ‰` bits where `d` is the number of dimensions and `c` is the cardinality of each. Otherwise, to index dimensions whose cardinality varies, we need a bitvector large enough to hold the concatenation. E.g., if the first dimension has cardinality `7`, we give it three bits, and the second has cardinality `3` we give it two bits and if the third dimension has cardinality `(2â¿+1, 2â¿âºÂ¹)` bits, we give it `n` bits, for a total of `s = 3 + 2 + n + 1`. Then we call `toIndexes(listOf(3, 2, n+1))` on a state to get a list of chunks indexing into our coordinate sets.

The careful reader may have noticed most chunks are unlikely to have a power-of-two-sized cardinality, thus we cannot form a bijection with the Galois field. This layout problem is a recurring issue in array programming: it is seldom the case we can allocate a bitvector which is an exact the number of bits we'll want to read from it. The usual solution is to pad the bitvector out to the next power of two, then read and write the bits as though they were an exact power of two. To preserve uniqueness, cryptographers invented something called the Hasty Pudding trick. No not [that Hasty Pudding ğŸ®](https://en.wikipedia.org/wiki/Hasty_pudding). [This hasty pudding ğŸ”](https://en.wikipedia.org/wiki/Hasty_Pudding_cipher).

Basically, we iterate the block cipher, throwing out any sample which contains a chunk exceeding the maximum cardinality in any indexed dimension until each contiguous subsequence of the bit vector is witnessed by a corresponding index in our coordinate set. In other words, we sample `b ~ ğ”¹Ë¢` from the codomain, then retain only those samples with a valid preimage for every chunk. This operation will preserve the injection.

```kotlin
// Discards samples exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>) =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

In the worst case, we will need to reject at most `~(Â½)áµˆ` samples. This can be a problem in very high dimensions whose cardinalities are all far from powers of two, e.g. `(2â¿ + 1)áµˆ where n, d->âˆ`, but even then, the cost of rejection will be dwarfed by the downstream costs of checking. There are higher radix Galois fields `GF(páµ)` for any prime `p` (denser than `2â¿`) that would improve sample complexity for irregularly-sized chunks, however since the underlying sampler is extremely fast already, even if we must reject thousands of samples, the cost of rejection will be negligible at modern clock frequencies.

Putting this all together, we have two methods, one for sampling from spaces like `AÃ—AÃ—A` and one for spaces like `AÃ—BÃ—C` where `A, B, C` are dimensions of heterogeneous cardinality.

```kotlin
// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  // Shuffle coordinates to increase entropy of sampling
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Using LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR never generates all 0s
```

See [here](https://github.com/breandan/kaliningraph/blob/master/src/commonMain/kotlin/ai/hypergraph/kaliningraph/sampling/Samplers.kt) for the full multidimensional sampling example. Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() = (4..10).forEach { i ->
    val list = LFSR(i).toList()
    val distinct = list.distinct()
    println("$i: ${list.size + 1} / ${2.0.pow(i).toInt()}")
    assertEquals(2.0.pow(i).toInt(), list.size + 1)
    assertEquals(2.0.pow(i).toInt(), distinct.size + 1)
  }

@Test
fun testMDSampler() =
  ((4..6 step 2) * (4..6)).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3D space. This is kinda like driving past a cornfield in your car and peering through the rows. You're looking for interference patterns. Knuth talks about it in his book somewhere.

## Weighted sampling

You might be wondering how all this relates to probabilistic programming. Don't we want to construct distributions with certain shapes? It turns out, we can transform our unweighted sampler into a weighted one using something called *inversion sampling*, or the *Smirnov transform*. Suppose already have a weighted distribution and want to draw samples in proportion to it, i.e., whose distribution matches in the limit. 

To do so, we first take our sample space for each element, compute a running sum in the histogram up to its index, called a cumulative distribution function (CDF). When drawing a sample from our PRNG, out comes a number uniformly between 0 and 1 in increments of `1/|alphabet|`: this represents the inverse CDF or *quantile*. To map this number back into sample space, we take the number and search for the index containing the nearest value, and voilÃ ! the unique element indexed by that bucket is our sample.

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip().let { (keys, values) -> 
        generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

This is basically the same guess-and-check method we saw earlier with a twist - since we know the CDF is monotonically increasing, we can speed up convergence via binary search. However, it still requires ğ’ª(n) space and time to compute the initial CDF, then ğ’ª(log(n)) for each sample thereafter, which may be problematic if the sample space is very large. We can do better! The Alias method is a constant-time weighted random sampler which precomputes in ğ’ª(nÂ·log(n)), then lets us jump to the right spot in ğ’ª(1):

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  fun sample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How could we adapt this to weighted sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ğ’ª(1) time and ğ’ª(2â¿) space. This is only really practical for small sets. Maybe we could do it dynamically for larger sets and cache the results.

# Learning the weights

So far, we have discussed how to sample from distributions with a known shape. But how do we learn that shape to begin with? We stream a bunch of data. It turns out, we can do so in a completely online manner using algebra.

To sample from short sequences, we can use a one dimensional distribution of the character or word frequencies. This is okay for short sequences, but might require a very large dictionary. Also, it has no memory, so sampling is unlikely to produce any interesting text.

In order to sample longer sequences, we might want to incorporate some context, such as pairs of adjacent characters. To do so, we could build a two-dimensional histogram, sample the first symbol from the "marginal" distribution `P(Tâ‚=tâ‚)`, and the second from the "conditional" distribution `P(Tâ‚‚=tâ‚‚|Tâ‚=tâ‚)`, the probability of the second character being `tâ‚‚` given the preceding character was `tâ‚`. This data structure is called a Markov or transition matrix.

```
P(Tâ‚=tâ‚,Tâ‚‚=tâ‚‚) = P(Tâ‚‚=tâ‚‚|Tâ‚=tâ‚)P(Tâ‚=tâ‚)

String: abcbbbbccbaâ€¦
1   2   3   4   5   6   7   8   9   10  â€¦
Window: ab, bc, cb, bb, bb, bb, bc, cc, cb, ba, â€¦
Counts: 1 , 2 , 2 , 3 , 3 , 3 , 2 , 1 , 2 , 1 , â€¦
Transition matrix at index=10:
   a  b  c â€¦
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€
aâ”‚ 0  1  0
bâ”‚ 1  3  2
câ”‚ 0  1  1
â‹®          / 10
```

More generally, we might have longer windows containing triples or n-tuples of contiguous symbols. To represent longer contexts, we could record their probabilities into a multidimensional array or transition tensor, representing the probability of a subsequence `tâ‚tâ‚‚...tâ‚™`. This tensor is a probability distribution whose conditionals "slice" or disintegrate the tensor along a dimension, producing an n-1 dimensional hyperplane, the conditional probability of observing a given symbol in a given slot:

```P(Tâ‚=tâ‚,Tâ‚‚=tâ‚‚,â€¦,Tâ‚™=tâ‚™) = P(Tâ‚™=tâ‚™|Tâ‚™â‚‹â‚=tâ‚™â‚‹â‚, Tâ‚™â‚‹â‚‚=tâ‚™â‚‹â‚‚, â€¦,Tâ‚=tâ‚)```

where the tensor rank `n` is given by the context length, `Tâ‚...â‚™` are random variables and `tâ‚...â‚™` are their concrete instantiations. This tensor is a hypercube with shape `|alphabet|â¿` - Each entry identifies a unique subsequence of n symbols, and the probability of observing them in the same context. Note the exponential state space of this model - as n grows larger, this will quickly require a very large amount of space to represent.

The first improvement we could make is to sparsify the tensor, i.e., only record its nonzero entries in some sort of list-based data structure, or sparse dictionary. Suppose in the previous example where `n=2`, we only stored nonzero entries as a list of pairs of bigrams to their frequency. By doing so, we could reduce the space consumption by 1/3. We could further reduce the space by only storing duplicate frequencies once. This would improve the space consumption by a small factor for multimodal distributions.

```
Matrix               Sparse List           Bidirectional Map

   a  b  c â€¦                                                   
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€                                  (ab,ba,cc) <-> 1   
aâ”‚ 0  1  0       [(ab,1),                       (bc,cb) <-> 2   
bâ”‚ 1  3  2        (ba,1),(bb,3),(bc,2)             (bb) <-> 3   
câ”‚ 0  2  1               (cb,2),(cc,1)]            else  -> 0
```

If we were really worried about space, we can do even better! Since the prefixes `*b` and `*c` occur more than once, we could store the transition counts as a prefix tree of pairs, whose first entry records the prefix and second records its frequency. Like before, we could compress this into a DAG to deduplicate leaves with equal-frequency. This might be depicted as follows:

```
          Prefix Tree                         Prefix DAG
             (*,10)                              (*,10)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
(a,1)        (b,6)           (c,3)      aâ”€â”€â”  6â”€â”€b   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€c
  â”‚      â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”´â”€â”€â”     â”‚  â”‚  â”Œâ”€â”€â”¼â”€â”€â”€â”‚â”€â”€â”   â”Œâ”€â”´â”€â”  
(b,1)  (a,1) (b,3) (c,2)  (b,2) (c,1)   b  â”‚  a  b   â”‚  c   b   c   
                                        â”‚  â”œâ”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”‚â”€â”€â”€â”˜
                                        â””â”€â”€â”¼â”€â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  
                                           1       3      2  
```

Space complexity, however important, is only part of the picture. Often the limiting factor in many data structures is the maximum speedup of parallelization. While concurrent tries and dictionaries are available, they are nontrivial to implement and have suboptimal scaling properties. A much more trivially scalable approach would be to recursively decompose the data into many disjoint subsets, summarize each one, and recombine the summaries. By designing the summary carefully, this process can be made embarrassingly parallel.

Mathematically, the structure we are looking for is something called a monoid. If the summary of interest can be computed in any order it is called a commutative monoid. Many summaries naturally exhibit this property: sum, min, max, top-k and various probability distributions. Summaries which can be decomposed and recombined in this fashion are embarrassingly parallelizable.

```
                             abcbbbbccba â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
            abcbbb                              bbccba                               â”‚
            (*,5)              +                (*,5)              =               (*,10)                  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
(a,1)       (b,3)       (c,1)  +        (b,3)            (c,2)     =  (a,1)        (b,6)           (c,3)   
  â”‚        â”Œâ”€â”€â”´â”€â”€â”        â”‚         â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”´â”€â”€â”         â”‚      â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”´â”€â”€â”  
(b,1)    (b,2) (c,1)    (b,1)  +  (a,1) (b,1) (c,1)   (b,1) (c,1)  =  (b,1)  (a,1) (b,3) (c,2)  (b,2) (c,1)
```

So far, we have considered exact methods. What if we didn't care about estimating the exact transition probability, but only approximating it. How could we achieve that? Perhaps by using a probabilistic data structure, we could reduce the complexity even further.

One approach would be to use an approximate counting algorithm, or sketch-based summary. Sketches are probabilistic datastructures for approximately computing some statistic efficiently. Without going into the details, sketching algorithms are designed to smoothly trade off error-bounds for space-efficiency and can be used to compute a summary statistic over a very large number of items. Even with a very low error tolerance, we can often obtain dramatic reduction in space complexity.

What about sample complexity? In many cases, we are not constrained by space or time, but samples. In many high-dimensional settings, even if we had an optimally-efficient sparse encoding, obtaining a faithful approximation to the true distribution would require more data than we could plausibly obtain. How could we do better in terms of sample efficiency? We need two things: (1) inductive priors and (2) learnable parameters. This is where algebraic structure, like groups, rings and their cousins come in handy.

If we squint a little, neural networks are a bit like a mergable summaries which deconstruct their inputs and recombine them in specific ways. For images, we have the special Euclidean group, SE(2). There are many other groups which are interesting to consider in various domains. By constructing our models with these invariants, we can recover latent structure with far, far fewer samples than would be required by a naive encoding scheme. For our purposes, we are particularly interested in semiring algebras, a specific kind of algebra that may be employed to compute many useful properties about graphs such as their longest, shortest and widest paths.

## Remarks

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. We then adapted it to multidimensional sampling with heterogeneous-length bit strings without compromising the no-replacement criterion. Using our PRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.

The LFSR is not a perfect random number generator, but for all intents and purposes it is pretty darn good. Implementing all this from scratch makes me wonder whether randomness really exists. Certainly probability is a useful concept, but seeing how [pseudo]randomness can emerge from a completely deterministic (albeit convoluted) process makes me think that maybe, if only somehow we could access the characteristic polynomial, we could reverse-engineer any apparently random process given enough time and space. Taking the computer science perspective a bit too far, it's tempting to think that maybe everything which appears random is actually a discrete dynamical system, and the purpose of consciousness is to reconstruct the characteristic polynomial.

On the practical end of the spectrum, I strongly suspect it is possible to use large language models to invert CSPRNGs. I'm 100% certain state actors have tried this. Weak ciphers are definitely within reach. They say public crypto is 10-20 years behind and the walls are starting to show cracks -- there is probably a reason why NIST is switching to quantum resistant cryptosystems. Given enough keystream data, all ciphers leak internal state -- it's certainly possible for the LFSR, LCG and other finite fields. If you're hiding state secrets, definitely go with an OTP with a quantum RNG. Otherwise, if you're just doing probabilistic programming and want decent random numbers as fast as possible, use an LFSR.
---
layout: post
title: Sampling without replacement in ùí™(1) space and time

---

A common task in probabilistic programming is to generate samples from a space whose cardinality is much larger than the number of bits available to process it in memory. What if someone told you that regardless of the size of your sample space, you can draw samples in constant space and time, whilst never producing the same sample twice? This is made possible by a beautiful theory developed by a young French mathematician named √âvariste Galois in the early 19th century. Today we can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!


## A na√Øve approach

A common problem in computer science is to generate some solutions meeting a set of criteria. Sometimes solutions can be produced directly using a clever algorithm, but in many cases, either no algorithm exists or is known of which can construct solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Beneath this seemingly simple idea lies some surprisingly intricate theory.

Suppose we have a fixed deadline and want to maximize the number of solutions found:

```
maximize solutions found in 5 minutes
```

We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(
    guesser: () -> List<T>,
    checker: (T) -> Boolean
): List<T> = guesser().filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we iterate over all items in the list filtering for the criteria, and only then do we return the results. There are a few problems here: first the list might not fit it in memory! And second, even if it could, filtering the entire space could take more time than we have to spare.

## A better approach

Instead, we will enumerate the guesses in a `Sequence`.

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    stop: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < stop }
```

This way, as soon as we have a guess, we can check it immediately.

```kotlin
val results = search(guesser, checker) // No computation performed
val list = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)`. Only when we pull from the sequence by calling `toList()` does this actually perform computation.

## Guessing

We could just enumerate items in lexicographic order, but what if the good results are all concentrated at the end, or in the middle, or in some specific spots? We could look in a specific order or check certain spots first, but what if we had no prior knowledge about where to search? Best we assume good answers are equidistributed across the whole space. Any ordering we choose could be suboptimal for a certain problem, so the best we can do is to search uniformly.

What's the difference between an order we choose and a random order? At first glance, uniform sampling might not seem all that useful as there are much more intelligent algorithms if the distribution is structured. But this is the magic of machine learning. We can choose any random projection and the algorithm still learns. The key property we want from a random sampler is that it explores all regions as early possible and produces a steady stream of results, not all clumped together in some places and bad in others. If there are clumpy spots which all pass or fail the test (unless they are constructed adversarially), it will produce them roughly proportional to their density. Wherever the data distribution lies, we will find it eventually. Later we can think about adaptive sampling, but first we must *explore*.

How do we sample a random order? We could use a Fisher-Yates shuffle. But that requires we store the entire space in memory, so we're back to square one. What if we drew a random integer representing the bit pattern of the object and construct it directly?

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt()) } }
    .map { i: Int -> makeExample(i) }
```

There are two problems here: first, it could draw duplicates. This might not be a big problem if the sample space is very large, but becomes more problematic if we want to sample without replacement from small sets. Second, if the sample space is large, the problem is, `Random.nextInt()` is limited to 2^32 or 2^64. Even if we could generate much larger numbers, the size of the spaces we are dealing with could be much, much larger. We want to be able to sample without replacement from small, medium and large spaces alike.

We could keep around a table of visited items and reject samples that intersect it:

```kotlin
fun <T> guesser(seen: MutableSet<Int> = mutableSetOf()) =
    sequence { while (true) { yield(Random.nextInt()) } }
        .filter { it !in seen } // Here we can also write .distinct()
```

This solves the issue of duplicates and runs in nearly constant time at first, but takes up an increasing about of time and space and space the longer it runs. While there are probabilistic datastructures like Bloom filters to mitigate spatial growth and still give high accurate membership queries, these do not address the issue of rejection sampling. For medium- to large-cardinality sets which are still tractable to search through exhaustively, this procedure would take much longer than enumerative search to visit all the elements. We can do much better. 

[//]: # (One key property useful things are enumerable. There are good space filling curves and Gray codes.)

To draw random samples without replacement, we need to understand where random numbers come from in the first place. The best RNGs come from nature. If we need truly irreversible random numbers, you'll want a quantum sensor. Short of that, the next best thing is a PRNG. The nice thing about is PRNG is that appears random but is a completely deterministic thing. Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially cellular automata. Below is a simple cellular automata, Boolean dynamical system, or pseudorandom number generator.

```
a  b  c  d  e     s1
1  0  0  0  0     s2
0  1  0  0  0  *  s3
0  0  1  0  0     s4
0  0  0  1  0     s5
```

It can be implemented in Kotlin as follows:

```kotlin
val algebra = Ring.of(
    nil = false,
    one = true,
    plus = { x, y -> x xor y },
    times = { x, y -> x and y }
)

private fun RandomVector(
    degree: Int,
    initialValue: UInt = 
        Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt(),
    initialState: List<Boolean> = initialValue.toBitList(degree),
) = FreeMatrix(algebra, degree, 1) { r, _ -> initialState[r] }

private fun TransitionMatrix(degree: Int, polynomial: List<Boolean>) =
    FreeMatrix(algebra, degree) { r, c -> 
        if (r == 0) polynomial[c] else c == r - 1 
    }

private fun PrimitivePolynomial(length: Int) =
    generator[length]!!.first().toString(2).map { it == '1' }

fun LFSRM(
    degree: Int,
    initialVec: FreeMatrix<Boolean> = RandomVector(degree),
    polynomial: List<Boolean> = PrimitivePolynomial(degree),
    trMat: FreeMatrix<Boolean> = TransitionMatrix(degree, polynomial)
): Sequence<UInt> = sequence {
    var i = 0
    var s: FreeMatrix<Boolean> = initialVec
    do {
        s = trMat * s
        yield(s.data.toUInt())
    } while (++i < 2.0.pow(degree).toInt() - 1)
}
```

This is equivalant to the GF(2^5), since `xor` is `mod 2` in binary.

It corresponds to the following machine:

```
 <---‚äï<----‚äï<----‚äï 
 |   ^     ^     ^
 v   |     |     |
 1-->0  1  0  1  1
 
s' = 1  0  1  0  1
s' = 0  1  0  1  0
s' = 0  0  1  0  1
...
```

It can be visualized in linear form:

...

It is a polynomial over GF(2). 

...

How do we solve for the coefficients?

By selecting the coefficients `a`..`e` carefully, we can ensure the period will be full. This means the state vector will not repeat itself for `2^5 - 1` time steps, i.e., the cardinality of the underlying set. Be assured this fact is no mere coincidence.

## Galois fields

Galois fields have a lot of applications in number theory, cryptography, and as it turns out, probabilistic programming.

Galois fields are sets, together with two operations <GF, +, *> which meet the following properties:

- **Closure**: The result of any operation on members of the Galois field is another member of the Galois field
- **Additive and multiplicative**: We can add matched parentheses around additive and multiplicative sequences wherever we like, i.e., `a*(b*c) = (a*b)*c`, `a+(b+c) = (a+b)+c`
- **Additive and multiplicative commutativity**: The order of the addition does not matter, nor does the order of multiplication, i.e., `a*b = b*a`, `a+b = b+a`
- **Distributivity**: Multiplication distributes over addition, i.e. `a*(b+c) = a * b + a * c`
- **Additive and multiplicative identity**: There is an element `0` s.t. `0 + a = a` and `1`, `1 != 0`, s.t. `1 * a = a`
- **Additive and multiplicative inverse**: For each `a` there is an element `-a` such that `a + (-a) = 0`. Similarly, for each nonzero `a` there is an element `1 / a` such that `a * (1 / a) = 1`

Galois fields have an interesting property: all nonzero elements have a unique inverse.

There is a generalized version for GF(p^k).

We can construct Galois fields by finding a generator for the underlying set. The generator will be a separable integer with a divisor of `1` and a primitive root of `2`. This can be accomplished by finding the smallest integer that meets the following criteria: ...

We need to search for the factors. Usually these are precomputed.

Below is an excerpt from Galois' last memoirs, written during his late teenage years, shortly before his untimely death at age 20.

<div class="quote"><blockquote><b>Pr√©face pour ‚ÄúDeux M√©moires d‚ÄôAnalyse Pure‚Äù</b> ‚Äì √âvariste Galois, 1811-1832 apr√®s J.-C.<br>

Les longs calculs alg√©briques ont d‚Äôabord √©t√© peu n√©cessaires au progr√®s des math√©matiques, les th√©or√®mes fort simples gagnaient √† peine √† √™tre traduits dans la langue de l‚Äôanalyse. Ce n‚Äôest gu√®re que depuis Euler que cette langue plus br√®ve est devenue indispensable √† la nouvelle extension que ce grand g√©om√®tre a donn√© √† la science. Depuis Euler, les calculs sont devenus de plus en plus n√©cessaires, mais de plus en plus difficiles... l‚Äôalgorithme avait atteint un degr√© de complication tel que tout progr√®s √©tait devenu impossible par ce moyen, sans l‚Äô√©l√©gance que les g√©om√®tres modernes ont su imprimer √† leurs recherches et au moyen de laquelle l'esprit saisit promptement et d‚Äôun seul coup un grand nombre d‚Äôop√©rations.

[...]

Il est √©vident que l‚Äô√©l√©gance, si vant√©e et √† si juste titre, n‚Äôa pas d‚Äôautre but.

[...]

Sauter √† pieds joints sur ces calculs; grouper les op√©rations, les classer suivant leurs difficult√©s et non suivant leurs formes; telle est, suivant moi, la mission des g√©om√®tres futurs; telle est la voie o√π je suis entr√© dans cet ouvrage.
</blockquote></div>

<div class="quote"><blockquote><b>Preface to ‚ÄúTwo Memoirs on Pure Analysis‚Äù</b> ‚Äì Evariste Galois, A.D. 1811-1832<br>

Long algebraic calculations were at first hardly necessary for progress in mathematics; the very simple theorems benefited little from being translated into the language of analysis. It is only since Euler this concision has become indispensable to continuing the work this great geometer has given to science. Since Euler, calculation has become more and more necessary and difficult... the algorithms so complicated that progress has become impossible without the elegance that modern geometers have brought to bear on their research, and by which means the mind can promptly and with a single glance grasp a large number of operations.

[...]

It is clear that elegance, so admired and so justly named, has no other purpose.

[...]

Jump with both feet into the calculations! Group the operations, classify them according to their difficulties and not according to their appearances. This, I believe, is the mission of future geometers. This is the road on which I am embarking in this work.
</blockquote></div>

## Multidimensional sampling

Let's take a closer look at the enumerative guesser.

```kotlin
fun <T> findAll(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
    findAll(List(dimension) { base })

fun <T> findAll(
    dimensions: List<Set<T>>,
    cardinalities: List<Int> = dimensions.map { it.size },
    asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> =
    all(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun all(i: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
    if (i.isEmpty()) sequenceOf(l)
    else (0 until i[0]).asSequence().flatMap { all(i.drop(1), l + it) }
```

But the problem is that the bit patterns are fixed. If we make a bad choice at the beginning, it might take a very long time to back out of it. See what it generates:

```
     v-Bad choice
    [1]00001
    [1]00010
    [1]00011
    [1]00100
```

Okay, so we can generate Booleans vectors. How do we map this back to the original vector space, whose cardinality might not be a power of two? One way might be to choose the leading bits, but that would destroy the involution, since the LFRS is only guaranteed to be unique up to the full degree polynomial. Instead, we will encode the string in chunks:

```kotlin
fun List<Int>.toBitLens() = map { ceil(log2(it.toDouble())).toInt() }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

val dimensions: List<Set<Char>> = 
    listOf('a'..'g', 'h'..'j', 'k'..'q', 'r'..'t').map { it.toSet() }
val bitLens = dimensions.map { it.size }.toBitLens()
val state = "1010101100".map { it == '1' }.toIndexes(bitLens)
// State is now [4, 1, 3, 0]
```

The chunks represent individual dimensions. So if the first dimension has cardinality 7, we give it three bits, and the second has cardinality 3 we give it two bits. Now we can call it with a state to get a list of integers, indexing into our coordinate sets. You might have noticed that these sets have a cardinality that is not a power of two. Thus, we cannot create a bijection with the Galois field. Oh no!

But fear not, we can use the Hasty Pudding trick. No not that Hasty Pudding. This hasty pudding:

https://en.wikipedia.org/wiki/Hasty_Pudding_cipher

Basically, we iterate the block cipher, throwing out any chunk which exceeds the maximum cardinality in any dimension until each contiguous subsequence in the bit vector is witnessed by a corresponding index in our coordinate set.

```kotlin
// Discards samples exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>) =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

This operation will preserve the injection. In the worst case, we will need to reject at most `~(1/2)^|cardinalities|` samples. This can be a problem in very high dimensions whose cardinalities are all far from powers of two, e.g. `(2^n + 1)^d where n, d->‚àû`. We could improve this bound by using a higher order Galois field on the next largest prime number, but since the underlying sampler is extremely fast and all modern hardware is binary anyway, even if we need to reject 1000s of samples on average, the cost of rejection will be dwarfed by the downstream costs of checking.

Putting the whole thing together, we have two methods, one for coordinate systems like B^n and one for things like AxBxC where A, B, C all have different cardinalities.

```kotlin
// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  // Shuffle coordinates to increase entropy of sampling
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Using LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR never generates all 0s
```

See [here](https://github.com/breandan/kaliningraph/blob/master/src/commonMain/kotlin/ai/hypergraph/kaliningraph/sampling/Samplers.kt) for the full multidimensional sampling example.

## Weighted sampling

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip().let { (keys, values) -> 
        generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

But we can do better! The Alias Method is a constant time weighted random sampler:

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  // Default sampler
  fun sample() = aliasSample()

  // Computes KS-transform using binary search
  fun bsSample(
    rng: Random = Random.Default,
    target: Double = rng.nextDouble()
  ): Int = cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }

  fun aliasSample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How do we adapt this to sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ùí™(1) time and ùí™(2^n) space. This is only really practical for small sets. We could do it dynamically for larger sets and cache the results.

## Testing

Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() = (4..10).forEach { i ->
    val list = LFSR(i).toList()
    val distinct = list.distinct()
    println("$i: ${list.size + 1} / ${2.0.pow(i).toInt()}")
    assertEquals(2.0.pow(i).toInt(), list.size + 1)
    assertEquals(2.0.pow(i).toInt(), distinct.size + 1)
  }

@Test
fun testMDSampler() =
  ((4..6 step 2).toSet() * (4..6).toSet()).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3d space. This is kinda like driving past a cornfield in your car and looking through the rows... Knuth talks about it in his book somewhere.

## Remarks

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. Using our PRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.

The LFSR is not a perfect random number generator, but for all intents and purposes it is pretty darn good. Implementing all this from scratch makes me wonder whether randomness really exists. Certainly probability is a useful concept, but seeing how randomness can emerge from a completely deterministic, albeit somewhat convoluted process makes me think that maybe if we had a large enough computer and access to the generating polynomial we could generate the universe. Perhaps that's taking the computer science perspective too far, but still, it's tempting to think that maybe everything is actually a Boolean dynamical system if we look close enough, and if we could just reconstruct its generating coefficients, we could reverse engineer any apparently random process.

I strongly suspect it is possible to use large language models to invert CSPRNGs. I'm 100% certain state actors have tried this. Weak ciphers are definitely in reach. They say public crypto is 10-20 years behind and empirical results cast doubt on the existence of trapdoor functions -- there's probably a reason why NIST is switching to quantum resistant cryptosystems. Given enough keystream data, all ciphers leak internal state -- it's definitely possible for the LFSR, LCG and other finite fields. If you're hiding state secrets, definitely go with an OTP with a quantum RNG. Otherwise, if you're just doing probabilistic programming and want decent random numbers as fast as possible, use an LFSR.
---
layout: post
title: Sampling without replacement in ùí™(1) space and time

---

A common task in probabilistic programming is to generate samples from a space whose cardinality is much larger than the number of bits available to process it in memory. What if someone told you that regardless of the size of your sample space, you can draw samples in constant space and time, whilst never producing the same sample twice? This is made possible by a beautiful theory developed by a young French mathematician named √âvariste Galois in the early 19th century. Today we can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!

## A na√Øve approach

A common problem in computer science is to generate some solutions meeting a set of criteria. Sometimes solutions can be produced directly using a clever algorithm, but in many cases, either no algorithm exists or is known of which can construct solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Beneath this seemingly simple idea lies some surprisingly intricate theory.

Suppose we have a fixed deadline and want to maximize the number of solutions found:

```
maximize solutions found in 5 minutes
```

We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(guesser: () -> List<T>, checker: (T) -> Boolean): List<T> = 
    guesser()/*All guesses must fit in memory*/.filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we iterate through all items in the list filtering for the criteria, and only then do we return the results. There are a few problems here: first the list might not fit it in memory! And second, even if it could, filtering the entire space could take more time than we have to spare.

## Continuations

Instead, we will enumerate the guesses in a `Sequence`. This way, only one `T` must fit in memory at any given point in time,  and as soon as we have a guess, we can check it immediately:

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    stop: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < stop }

val results = search(guesser, checker) // No computation performed
val list = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)`. Only when we pull from the sequence by calling `toList()` does this actually perform computation. This pattern is called a continuation.

## Guessing

We could just enumerate items in lexicographic order, but what if the good results are all concentrated at the end, or in the middle, or in specific spots? What if we had no prior knowledge about where to search? Best to assume good answers are equidistributed across the whole space. Any ordering could be suboptimal on a certain problem, so the best we can do is to search uniformly.

At first glance, uniform sampling might not seem all that useful as there are more efficient algorithms for sampling. We can think about adaptive sampling later, but first we must *explore*. The key property we want from a random sampler is that it explores all regions as early possible and produces a steady stream of results, not all clumped together in some places and sparse in others. If there are clumpy parts containing an abnormal ratio of good to bad guesses, that's bad. Our sampler should draw positive and negative samples with wall-clock frequency roughly proportional to their true density across the entire search space. Another way of putting it is that we want the sequence to be temporally invariant with respect to translation and scale: sufficiently long subsequences should all be equally representative of the entire sequence. This property is known as *ergodicity*.

How do we ensure this property holds? We could use a Fisher-Yates or Knuth shuffle. But that would require storing the entire search space in memory, so we're back to square one. What if we drew a random integer representing the bit pattern of the object and used that to constructed our guess directly?

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt()) } }
    .map { i: Int -> makeExample(i) }
```

There are two problems here: first, it could draw duplicates. That might not be a big issue if the sample space is very large, but becomes more problematic if we want to sample without replacement from small sets. Second, if the sample space is large, `Random.nextInt()` is limited to 2^32 or 2^64. Even if we could generate much larger numbers, the size of the spaces we are dealing with could be much, much larger. We want a single procedure to sample without replacement from small, medium and large spaces alike in constant time.

We could keep around a table of visited items and reject samples that intersect it:

```kotlin
fun <T> guesser(seen: MutableSet<Int> = mutableSetOf()) =
    sequence { while (true) { yield(Random.nextInt()) } }
        .filter { it !in seen } // Here we can also write .distinct()
```

This solves the issue of duplicates and runs in nearly constant time at first, but consumes an increasing about of time and space the more samples are drawn. While there are probabilistic datastructures like Bloom filters to mitigate spatial growth and provide high-accuracy membership queries on large-cardinality sets, these do not address the issue of rejection sampling. For small and medium to large cardinality sets which may be intractable to store in memory but still tractable to exhaustively search, we want a unified procedure. Let's revisit our criteria again:

```
1.) Eventually visits all elements in the search space
2.) Time shift and scale invariant, i.e., ergodic
3.) Does not repeat any samples before halting
4.) Consumes constant time and space per sample
5.) Determinstical halts after fixed time elapsed
6.) Reproducible given the same initial seed
7.) Scales to arbitrarily large search spaces
```

It turns out all of the above criteria are equisatisfiable. We will now describe an elegant procedure for doing so.

To draw random samples without replacement, we need to understand where random numbers come from in the first place. The best RNGs come from nature. For truly irreversible random numbers, you'll want a quantum sensor. Short of that, the next best thing is a function whose outputs are statistically indistinguishable from RNGs but are a completely deterministic given the starting configuration, so we can reproduce it later should the need arise. Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially discrete dynamical systems called cellular automata. Below is a simple cellular automata, Boolean dynamical system, or pseudorandom number generator.

```
a  b  c  d  e     s0
1  0  0  0  0     s1
0  1  0  0  0  *  s2
0  0  1  0  0     s3
0  0  0  1  0     s4

      M        *  S
```

It can be implemented in Kotlin as follows:

```kotlin
val algebra = Ring.of(
    nil = false,
    one = true,
    plus = { x, y -> x xor y },
    times = { x, y -> x and y }
)

private fun RandomVector(
    degree: Int,
    initialValue: UInt = 
        Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt(),
    initialState: List<Boolean> = initialValue.toBitList(degree),
) = FreeMatrix(algebra, degree, 1) { r, _ -> initialState[r] }

private fun TransitionMatrix(degree: Int, polynomial: List<Boolean>) =
    FreeMatrix(algebra, degree) { r, c -> 
        if (r == 0) polynomial[c] else c == r - 1 
    }

private fun PrimitivePolynomial(length: Int) =
    generator[length]!!.first().toString(2).map { it == '1' }

fun LFSRM(
    degree: Int,
    initialVec: FreeMatrix<Boolean> = RandomVector(degree),
    polynomial: List<Boolean> = PrimitivePolynomial(degree),
    trMat: FreeMatrix<Boolean> = TransitionMatrix(degree, polynomial)
): Sequence<UInt> = sequence {
    var i = 0
    var s: FreeMatrix<Boolean> = initialVec
    do {
        s = trMat * s
        yield(s.data.toUInt())
    } while (++i < 2.0.pow(degree).toInt() - 1)
}
```

The astute reader might note that since we are using a matrix, the time and space complexity is not constant. However, we note that the matrix is sparse, and computing the matrix-vector product can be reduced to exactly six bitwise operations. It can be visualized as the following circuit:

```
 <-------‚äï<----‚äï<----‚äï 
 |       ^     ^     ^
 v       |     |     |
 1--->_  0  1  0  1  1
 
s' =     1  0  1  0  1
s' =     0  1  0  1  0
s' =     0  0  1  0  1
...
```

By selecting the coefficients `a`..`e` carefully, we can ensure the period will be full. For example, if we use the coefficients above, i.e. `[1 0 1 0 1]`,  the state vector will not repeat itself for `31` time steps, i.e., the cardinality of the underlying set. Be assured, dear reader, this fact is no mere coincidence.

## Galois fields

A field is an algebraic structure `<F, +, *>` with the following properties:

- **Closure**: The result of any operation on members of the Galois field is another member of the Galois field
- **Additive and multiplicative**: We can add matched parentheses around additive and multiplicative sequences wherever we like, i.e., `a*(b*c) = (a*b)*c`, `a+(b+c) = (a+b)+c`
- **Additive and multiplicative commutativity**: The order of the addition does not matter, nor does the order of multiplication, i.e., `a*b = b*a`, `a+b = b+a`
- **Distributivity**: Multiplication distributes over addition, i.e. `a*(b+c) = a * b + a * c`
- **Additive and multiplicative identity**: There is an element `0` s.t. `0 + a = a` and `1`, `1 != 0`, s.t. `1 * a = a`
- **Additive and multiplicative inverse**: For each `a` there is an element `-a` such that `a + (-a) = 0`. Similarly, for each nonzero `a` there is an element `1 / a` such that `a * (1 / a) = 1`

Galois fields are fields with a finite number of elements. They have lots of applications in number theory, cryptography, and as it turns out, probabilistic programming. We can describe Galois fields of degree `2^n` using a special kind of function that maps the field back onto itself called a feedback polynomial. A subset of these, called primitive polynomials, have the envious property of being minimal and generating a maximal-length period. These two properties make it highly desirable: a primitive polynomial on `GF(2^n)` can generate the entire field using `~O(log(n))` bits.

This means the primitive polynomial is a space-filling curve, i.e., a curve which visits all `2^n-1` elements of the set exactly once before repeating. However, unlike simple enumerative procedures like Hilbert Curves or Gray Codes, the PP takes a nontrivial trajectory through its state space making it very difficult to predict where the function will take us more than a few timesteps ahead. (Although not impossible in an adversarial setting, more sophisticated versions exist that make the LFRS resistant to inversion albeit increasing the computational costs.)

In particular, we can use the primitive polynomial to construct a finite state machine or linear finite state register (LFSR) whose intermediate states are all nearly indistinguishable from a true RNG on many summary statistics. This makes them useful proxies for simulating stochasticity on a deterministic machine. Their maximal period allows us to generate the entire set by cycling through its primitive polynomial, a property which makes it useful for unbiased sampling without replacement.

How do we find primitive polynomials? We could simply enumerate all polynomials up to `log_p(|S|)` and check whether they are full period. It so happens we have already seen a guess-and-check algorithm that can generate examples in random order. This works for low-degree polynomials but quickly becomes intractable with increasing degree.

The beauty of the primitive polynomials over GF(2) is that they can be composed using other primitive polynomials. This fact allows to write a decision procedure that eliminates certain guesses using a static property of the polynomial rather than measuring its periodicity by simulating the dynamics, which might not terminate before the heat death of the universe.

A degree-r polynomial is primitive iff its period is `2^r - 1`...

It turns out we can use the same matrix multiplication procedure we used to simulate the dynamics, to find solutions for the statics.

Below is an excerpt from Galois' last memoirs, written during his late teenage years, shortly before his untimely death at age 20.

<div class="quote"><blockquote><b>Pr√©face pour ‚ÄúDeux M√©moires d‚ÄôAnalyse Pure‚Äù</b> ‚Äì √âvariste Galois, 1811-1832 apr√®s J.-C.<br>

Les longs calculs alg√©briques ont d‚Äôabord √©t√© peu n√©cessaires au progr√®s des math√©matiques, les th√©or√®mes fort simples gagnaient √† peine √† √™tre traduits dans la langue de l‚Äôanalyse. Ce n‚Äôest gu√®re que depuis Euler que cette langue plus br√®ve est devenue indispensable √† la nouvelle extension que ce grand g√©om√®tre a donn√© √† la science. Depuis Euler, les calculs sont devenus de plus en plus n√©cessaires, mais de plus en plus difficiles... l‚Äôalgorithme avait atteint un degr√© de complication tel que tout progr√®s √©tait devenu impossible par ce moyen, sans l‚Äô√©l√©gance que les g√©om√®tres modernes ont su imprimer √† leurs recherches et au moyen de laquelle l'esprit saisit promptement et d‚Äôun seul coup un grand nombre d‚Äôop√©rations.

[...]

Il est √©vident que l‚Äô√©l√©gance, si vant√©e et √† si juste titre, n‚Äôa pas d‚Äôautre but.

[...]

Sauter √† pieds joints sur ces calculs; grouper les op√©rations, les classer suivant leurs difficult√©s et non suivant leurs formes; telle est, suivant moi, la mission des g√©om√®tres futurs; telle est la voie o√π je suis entr√© dans cet ouvrage.
</blockquote></div>

<div class="quote"><blockquote><b>Preface to ‚ÄúTwo Memoirs on Pure Analysis‚Äù</b> ‚Äì Evariste Galois, A.D. 1811-1832<br>

Long algebraic calculations were at first hardly necessary for progress in mathematics; the very simple theorems benefited little from being translated into the language of analysis. It is only since Euler this concision has become indispensable to continuing the work this great geometer has given to science. Since Euler, calculation has become more and more necessary and difficult... the algorithms so complicated that progress has become impossible without the elegance that modern geometers have brought to bear on their research, and by which means the mind can promptly and with a single glance grasp a large number of operations.

[...]

It is clear that elegance, so admired and so justly named, has no other purpose.

[...]

Jump with both feet into the calculations! Group the operations, classify them according to their difficulties and not according to their appearances. This, I believe, is the mission of future geometers. This is the road on which I am embarking in this work.
</blockquote></div>

## Multidimensional sampling

Let's take a closer look at the enumerative guesser.

```kotlin
fun <T> findAll(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
    findAll(List(dimension) { base })

fun <T> findAll(
    dimensions: List<Set<T>>,
    cardinalities: List<Int> = dimensions.map { it.size },
    asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> =
    all(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun all(i: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
    if (i.isEmpty()) sequenceOf(l)
    else (0 until i[0]).asSequence().flatMap { all(i.drop(1), l + it) }
```

But the problem is that the bit patterns are fixed. If we make a bad choice at the beginning, it might take a very long time to back out of it. See what it generates:

```
     v-Bad choice
    [1]00001
    [1]00010
    [1]00011
    [1]00100
```

Okay, so we can generate Booleans vectors. How do we map this back to the original vector space, whose cardinality might not be a power of two? One way might be to choose the leading bits, but that would destroy the involution, since the LFRS is only guaranteed to be unique up to the full degree polynomial. Instead, we will encode the string in chunks:

```kotlin
fun List<Int>.toBitLens() = map { ceil(log2(it.toDouble())).toInt() }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

val dimensions: List<Set<Char>> = 
    listOf('a'..'g', 'h'..'j', 'k'..'q', 'r'..'t').map { it.toSet() }
val bitLens = dimensions.map { it.size }.toBitLens()
val state = "1010101100".map { it == '1' }.toIndexes(bitLens)
// State is now [4, 1, 3, 0]
```

The chunks represent individual dimensions. So if the first dimension has cardinality 7, we give it three bits, and the second has cardinality 3 we give it two bits. Now we can call it with a state to get a list of integers, indexing into our coordinate sets. You might have noticed that these sets have a cardinality that is not a power of two. Thus, we cannot create a bijection with the Galois field. Oh no!

But fear not, we can use the Hasty Pudding trick. No not that Hasty Pudding. This hasty pudding:

https://en.wikipedia.org/wiki/Hasty_Pudding_cipher

Basically, we iterate the block cipher, throwing out any chunk which exceeds the maximum cardinality in any dimension until each contiguous subsequence in the bit vector is witnessed by a corresponding index in our coordinate set.

```kotlin
// Discards samples exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>) =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

This operation will preserve the injection. In the worst case, we will need to reject at most `~(1/2)^|cardinalities|` samples. This can be a problem in very high dimensions whose cardinalities are all far from powers of two, e.g. `(2^n + 1)^d where n, d->‚àû`, but in almost all cases where sampling without replacement is a concern, the cost of rejection will be dwarfed by the downstream costs of checking. There are higher radix Galois fields   `GF(p^k)` for any prime `p`, and since the gaps between consecutive primes are much smaller than `2^n` this would improve sample complexity. But as the underlying sampler is extremely fast already and all digital hardware is binary-valued, even if we must reject thousands of samples, the cost of rejection will be negligible at modern clock frequencies.

Putting the whole thing together, we have two methods, one for coordinate systems like B^n and one for things like AxBxC where A, B, C all have different cardinalities.

```kotlin
// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  // Shuffle coordinates to increase entropy of sampling
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Using LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR never generates all 0s
```

See [here](https://github.com/breandan/kaliningraph/blob/master/src/commonMain/kotlin/ai/hypergraph/kaliningraph/sampling/Samplers.kt) for the full multidimensional sampling example.

## Weighted sampling

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip().let { (keys, values) -> 
        generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

But we can do better! The Alias Method is a constant time weighted random sampler:

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  // Default sampler
  fun sample() = aliasSample()

  // Computes KS-transform using binary search
  fun bsSample(
    rng: Random = Random.Default,
    target: Double = rng.nextDouble()
  ): Int = cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }

  fun aliasSample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How do we adapt this to sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ùí™(1) time and ùí™(2^n) space. This is only really practical for small sets. We could do it dynamically for larger sets and cache the results.

## Testing

Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() = (4..10).forEach { i ->
    val list = LFSR(i).toList()
    val distinct = list.distinct()
    println("$i: ${list.size + 1} / ${2.0.pow(i).toInt()}")
    assertEquals(2.0.pow(i).toInt(), list.size + 1)
    assertEquals(2.0.pow(i).toInt(), distinct.size + 1)
  }

@Test
fun testMDSampler() =
  ((4..6 step 2).toSet() * (4..6).toSet()).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3d space. This is kinda like driving past a cornfield in your car and looking through the rows... Knuth talks about it in his book somewhere.

## Remarks

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. Using our PRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.

The LFSR is not a perfect random number generator, but for all intents and purposes it is pretty darn good. Implementing all this from scratch makes me wonder whether randomness really exists. Certainly probability is a useful concept, but seeing how randomness can emerge from a completely deterministic, albeit somewhat convoluted process makes me think that maybe if we had a large enough computer and access to the generating polynomial we could generate the universe. Perhaps that's taking the computer science perspective too far, but still, it's tempting to think that maybe everything is actually a Boolean dynamical system if we look close enough, and if we could just reconstruct its generating coefficients, we could reverse engineer any apparently random process.

I strongly suspect it is possible to use large language models to invert CSPRNGs. I'm 100% certain state actors have tried this. Weak ciphers are definitely in reach. They say public crypto is 10-20 years behind and empirical results cast doubt on the existence of trapdoor functions -- there's probably a reason why NIST is switching to quantum resistant cryptosystems. Given enough keystream data, all ciphers leak internal state -- it's definitely possible for the LFSR, LCG and other finite fields. If you're hiding state secrets, definitely go with an OTP with a quantum RNG. Otherwise, if you're just doing probabilistic programming and want decent random numbers as fast as possible, use an LFSR.
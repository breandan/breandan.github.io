---
layout: post
title: Sampling without replacement in ùí™(1) space and time

---

A common task in probabilistic programming is to generate samples from a space whose cardinality is much larger than the number of bits available to process it in memory. What if someone told you that regardless of the size of your sample space, you can draw samples in constant space and time, while never producing the same sample twice? This is made possible by a beautiful theory developed by a young French mathematician named √âvariste Galois in the early 19th century. Today you can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!

## A na√Øve approach

A common problem in computer science is to generate some solutions meeting a set of criteria. Sometimes solutions can be produced directly using a clever algorithm, but in many cases, either no algorithm exists or is known of which can construct solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Beneath this seemingly simple idea lies some surprisingly intricate theory.

Suppose we have a fixed deadline and want to maximize the number of solutions found:

```
maximize solutions found in 5 minutes
```

We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(
    guesser: () -> List<T>,
    checker: (T) -> Boolean
): List<T> = guesser().filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we iterate over all items in the list filtering for the criteria, and only then do we return the results. There are a few problems here: first the list might not fit it in memory! And second, even if it could, filtering the entire space could take more time than we have to spare.

## A better approach

Instead, we will enumerate the guesses in a `Sequence`.

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    deadline: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < deadline }
```

This way, as soon as we have a guess, we can check it immediately.

```kotlin
val results = search(guesser, checker) //No computation performed
val list = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)`. Only when we pull from the sequence by calling `toList()` does this actually perform computation.

## Guessing

We could just enumerate items in lexicographic order, but what if the good results are all concentrated at the end, or in the middle, or in some specific spots? We could look in a specific order or check certain spots first, but what if we had no prior knowledge about where to search? Best we assume good answers are equidistributed across the whole space. Any ordering we choose could be suboptimal for a certain problem, so the best we can do is to search uniformly. We could have an adaptive sampler, but that's a topic for another post.

We could use a Fisher-Yates shuffle. But that requires we store everything in memory, so we're back to square one. What if we drew a random integer representing the bit pattern of the object and construct it directly. Many useful things are enumerable. There are good space filling curves and Gray codes. It's worth a try.

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt()) } }
    .map { i: Int -> makeExample(i) }
```

There are two problems here: first, it could draw duplicates. This might not be a big problem if the sample space is very large, but becomes more problematic if we want to sample without replacement from small sets. Second, if the sample space is large, the problem is, `Random.nextInt()` is limited to 2^32 or 2^64. Even if we could generate much larger numbers, the size of the spaces we are dealing with could be much, much larger. We want to be able to sample without replacement from small, medium and large spaces alike.

To draw random samples, we need understand where random numbers come from. The best random numbers come from nature. If we need truly irreversible random numbers, you need a quantum sensor. The problem with that is we also want it to be reproducible, i.e., deterministic. Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially cellular automata.

Below is a simple cellular automata, Boolean dynamical system, or pseudorandom number generator.

```
a  b  c  d  e     s1
1  0  0  0  0     s2
0  1  0  0  0  *  s3
0  0  1  0  0     s4
0  0  0  1  0     s5
```

It can be implemented in Kotlin:

```kotlin
fun LFSR(degree: Int = 16): Sequence<UInt> = sequence {
  val vec0 = Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt()
  var vec = vec0
  val taps = generator[degree]!!.random().toString(2)
    .mapIndexedNotNull { i, c -> if (c == '1') i else null }
  do {
    val bit = taps.map { vec shr it }.fold(0u) { a, c -> a xor c } and 1u
    vec = (vec shr 1) or (bit shl (degree - 1))
    yield(vec)
  } while (vec != vec0)
}
```

We can also use the matrix form with a matrix semiring:

```kotlin
val algebra = Ring.of(
    nil = false,
    one = true,
    plus = { x, y -> x or y },
    times = { x, y -> x xor y }
)

var s = Vector(false, true, false, true, true)
val m = BooleanMatrix(algebra, 5, 5) { /*...*/ }
val s = m * m * m * m * s//...
```

This is equivalant to the GF(2^5), since `xor` is `mod 2` in binary.

It corresponds to the following machine:

```
 <---‚äï<----‚äï<----‚äï 
 |   ^     ^     ^
 v   |     |     |
 1-->0  1  0  1  1
 
s' = 1  0  1  0  1
s' = 0  1  0  1  0
s' = 0  0  1  0  1
...
```

It can be visualized in linear form:

...

It is a polynomial over GF(2). 

...

How do we solve for the coefficients?

If we choose these coefficients `a`..`e`, carefully, the period will be full. This means the state vector will not repeat itself for 2^5, i.e. the cardinality of the underlying set.

## Math time

Galois fields have a lot of applications in number theory, cryptography, and as it turns out, probabilistic programming. 

There is a generalized version for GF(p^e). 

We need to search for the factors. Usually these are precomputed.

## Multidimensional sampling

Let's take a closer look at the enumerative guesser.

```kotlin
fun <T> exhaustiveSearch(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
  exhaustiveSearch(List(dimension) { base })

fun <T> exhaustiveSearch(
  dimensions: List<Set<T>>,
  cardinalities: List<Int> = dimensions.map { it.size },
  asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> = mls(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun mls(base: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
  if (base.isEmpty()) sequenceOf(l)
  else (0 until base[0]).asSequence().flatMap { mls(base.drop(1), l + it) }
```

But the problem is that the bit patterns are fixed. If we make a bad choice at the beginning, it might take a very long time to cycle out of it. See what it generates:

```
      -
    [1]00001
    [1]00010
    [1]00011
    [1]00100
```

Okay, so we can generate Booleans. How do we map this back to the original vector space?

We can use the Hasty Pudding trick. No not that Hasty Pudding. This hasty pudding:

https://en.wikipedia.org/wiki/Hasty_Pudding_cipher

Basically, we throw out anything which exceeds the maximum in any dimension:

```kotlin
// Discards samples representing an integer exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>): Sequence<List<Int>> =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

This operation will preserve the bijection. In the worst case, we will only need to reject at most `~(1/2)^|cardinalities|` samples. This can be a problem in very high dimensions, but since the underlying sampler is very fast, in practice rejection will be dwarfed by downstream costs of checking.

Putting the whole thing together:

```kotlin

val generator = mapOf(
    // Maps degree to binary polynomial coefficients in decimal form
    // https://link.springer.com/content/pdf/bbm%3A978-1-4615-1509-8%2F1.pdf
    4 to listOf(19, 25),
    5 to listOf(37, 41, 47, 55, 59, 61),
    6 to listOf(67, 91, 97, 103, 109, 115),
    //...
)
fun LFSR(degree: Int = 16): Sequence<UInt> = sequence {
  val vec0 = Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt()
  var vec = vec0
  val taps = generator[degree]!!.random().toString(2)
    .mapIndexedNotNull { i, c -> if (c == '1') i else null }
  do {
    val bit = taps.map { vec shr it }.fold(0u) { a, c -> a xor c } and 1u
    vec = (vec shr 1) or (bit shl (degree - 1))
    yield(vec)
  } while (vec != vec0)
}

// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Sampling with LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  if (degree !in generator) throw Exception("Space is too large!")
  else LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR will never generate all 0s

fun List<Int>.toBitLens(): List<Int> = map { ceil(log2(it.toDouble())).toInt() }
fun List<Boolean>.toInt() = joinToString("") { if(it) "1" else "0" }.toInt(2)
fun UInt.toBitList(len: Int): List<Boolean> =
  toString(2).padStart(len, '0').map { it == '1' }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

// Discards samples representing an integer exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>): Sequence<List<Int>> =
  map { it.toIndexes(cardinalities.toBitLens()) }
    .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

## Weighted sampling

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip()
    .let { (keys, values) -> generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

But we can do better! The Alias Method is a constant time weighted random sampler:

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  // Default sampler
  fun sample() = aliasSample()

  // Computes KS-transform using binary search
  fun bsSample(
    rng: Random = Random.Default,
    target: Double = rng.nextDouble()
  ): Int = cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }

  fun aliasSample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How do we adapt this to sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ùí™(1) time and ùí™(2^n) space. This is only really practical for small sets. We could do it dynamically for larger sets and cache the results.

## Testing

Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() {
  // Tests whether LFSR cycles through its maximal period
  for (i in 4..10) {
    val size = LFSR(i).toList().distinct().size
    println("$i: ${2.0.pow(i).toInt()} / ${size + 1}")
    assertEquals(2.0.pow(i).toInt(), size + 1)
  }
}

@Test
fun testMDSampler() {
  ((4..6 step 2).toSet() * (4..6).toSet()).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
}
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3d space. This is kinda like driving past a cornfield in your car and looking through the rows... Knuth talks about it in his book somewhere.

## Conclusion

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. Using our UPRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.
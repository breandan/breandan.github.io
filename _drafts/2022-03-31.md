---
layout: post
title: Sampling without replacement in ùí™(1) space and time

---

A common task in probabilistic programming is to generate samples from a space whose cardinality is much larger than the number of bits available to process it in memory. What if someone told you that regardless of the size of your sample space, you can draw samples in constant space and time, whilst never producing the same sample twice? This is made possible by a beautiful theory developed by a young French mathematician named √âvariste Galois in the early 19th century. Today we can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!

## A na√Øve approach

A common problem in computer science is to generate some solutions meeting a set of criteria. Sometimes solutions can be produced directly using a clever algorithm, but in many cases, either no algorithm exists or is known of which can construct solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Beneath this seemingly simple idea lies some surprisingly intricate theory.

Suppose we have a fixed deadline and want to maximize the number of solutions found:

```
maximize solutions found in 5 minutes
```

We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(
    guesser: () -> List<T>,
    checker: (T) -> Boolean
): List<T> = guesser().filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we iterate over all items in the list filtering for the criteria, and only then do we return the results. There are a few problems here: first the list might not fit it in memory! And second, even if it could, filtering the entire space could take more time than we have to spare.

## A better approach

Instead, we will enumerate the guesses in a `Sequence`.

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    deadline: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < deadline }
```

This way, as soon as we have a guess, we can check it immediately.

```kotlin
val results = search(guesser, checker) //No computation performed
val list = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)`. Only when we pull from the sequence by calling `toList()` does this actually perform computation.

## Guessing

We could just enumerate items in lexicographic order, but what if the good results are all concentrated at the end, or in the middle, or in some specific spots? We could look in a specific order or check certain spots first, but what if we had no prior knowledge about where to search? Best we assume good answers are equidistributed across the whole space. Any ordering we choose could be suboptimal for a certain problem, so the best we can do is to search uniformly.

What's the difference between an order we choose and a random order? At first glance, uniform sampling might not seem all that useful as there are much more intelligent algorithms if the distribution is structured. But this is the magic of machine learning. We can choose any random projection and the algorithm still learns. The key property we want from a random sampler is that it explores all regions as early possible and produces a steady stream of results, not all clumped together in some places and bad in others. If there are clumpy spots which all pass or fail the test (unless they are constructed adversarially), it will produce them roughly proportional to their density. Wherever the data distribution lies, we will find it eventually. Later we can think about adaptive sampling, but first we must *explore*.

How do we sample a random order? We could use a Fisher-Yates shuffle. But that requires we store the entire space in memory, so we're back to square one. What if we drew a random integer representing the bit pattern of the object and construct it directly?

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt()) } }
    .map { i: Int -> makeExample(i) }
```

There are two problems here: first, it could draw duplicates. This might not be a big problem if the sample space is very large, but becomes more problematic if we want to sample without replacement from small sets. Second, if the sample space is large, the problem is, `Random.nextInt()` is limited to 2^32 or 2^64. Even if we could generate much larger numbers, the size of the spaces we are dealing with could be much, much larger. We want to be able to sample without replacement from small, medium and large spaces alike.

We could keep around a table of visited items and reject samples that intersect it:

```kotlin
fun <T> guesser(seen: MutableSet<Int> = mutableSetOf()) =
    sequence { while (true) { yield(Random.nextInt()) } }
        .filter { it !in seen } // Here we can also write .distinct()
```

This solves the issue of duplicates and runs in nearly constant time at first, but takes up an increasing about of time and space and space the longer it runs. While there are probabilistic datastructures like Bloom filters to mitigate spatial growth and still give high accurate membership queries, these do not address the issue of rejection sampling. For medium- to large-cardinality sets which are still tractable to search through exhaustively, this procedure would take much longer than enumerative search to visit all the elements. We can do much better. 

[//]: # (One key property useful things are enumerable. There are good space filling curves and Gray codes.)

To draw random samples without replacement, we need to understand where random numbers come from in the first place. The best RNGs come from nature. If we need truly irreversible random numbers, you'll want a quantum sensor. Short of that, the next best thing is a PRNG. The nice thing about is PRNG is that appears random but is a completely deterministic thing. Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially cellular automata. Below is a simple cellular automata, Boolean dynamical system, or pseudorandom number generator.

```
a  b  c  d  e     s1
1  0  0  0  0     s2
0  1  0  0  0  *  s3
0  0  1  0  0     s4
0  0  0  1  0     s5
```

It can be implemented in Kotlin as follows:

```kotlin
val algebra = Ring.of(
    nil = false,
    one = true,
    plus = { x, y -> x xor y },
    times = { x, y -> x and y }
)

private fun RandomVector(
    degree: Int,
    initialValue: UInt = Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt(),
    initialState: List<Boolean> = initialValue.toBitList(degree),
) = FreeMatrix(algebra, degree, 1) { r, _ -> initialState[r] }

// https://en.wikipedia.org/wiki/Linear-feedback_shift_register#Matrix_forms
private fun TransitionMatrix(degree: Int, polynomial: List<Boolean>) =
    FreeMatrix(algebra, degree) { r, c -> if (r == 0) polynomial[c] else c == r - 1 }

private fun PrimitivePolynomial(length: Int) =
    generator[length]!!.first().toString(2).map { it == '1' }

fun LFSRM(
    degree: Int,
    initialVec: FreeMatrix<Boolean> = RandomVector(degree),
    primitivePolynomial: List<Boolean> = PrimitivePolynomial(degree),
    matrix: FreeMatrix<Boolean> = TransitionMatrix(degree, primitivePolynomial)
): Sequence<UInt> = sequence {
    var i = 0
    var s: FreeMatrix<Boolean> = initialVec
    do {
        s = matrix * s
        yield(s.data.toUInt())
    } while (++i < 2.0.pow(degree).toInt() - 1)
}
```

This is equivalant to the GF(2^5), since `xor` is `mod 2` in binary.

It corresponds to the following machine:

```
 <---‚äï<----‚äï<----‚äï 
 |   ^     ^     ^
 v   |     |     |
 1-->0  1  0  1  1
 
s' = 1  0  1  0  1
s' = 0  1  0  1  0
s' = 0  0  1  0  1
...
```

It can be visualized in linear form:

...

It is a polynomial over GF(2). 

...

How do we solve for the coefficients?

By selecting the coefficients `a`..`e` carefully, we can ensure the period will be full. This means the state vector will not repeat itself for `2^5 - 1` time steps, i.e., the cardinality of the underlying set. Be assured this fact is no mere coincidence.

## Math time

Galois fields have a lot of applications in number theory, cryptography, and as it turns out, probabilistic programming. 

There is a generalized version for GF(p^k). 

We need to search for the factors. Usually these are precomputed.

## Multidimensional sampling

Let's take a closer look at the enumerative guesser.

```kotlin
fun <T> exhaustiveSearch(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
  exhaustiveSearch(List(dimension) { base })

fun <T> exhaustiveSearch(
  dimensions: List<Set<T>>,
  cardinalities: List<Int> = dimensions.map { it.size },
  asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> = mls(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun mls(base: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
  if (base.isEmpty()) sequenceOf(l)
  else (0 until base[0]).asSequence().flatMap { mls(base.drop(1), l + it) }
```

But the problem is that the bit patterns are fixed. If we make a bad choice at the beginning, it might take a very long time to back out of it. See what it generates:

```
     v-Bad choice
    [1]00001
    [1]00010
    [1]00011
    [1]00100
```

Okay, so we can generate Booleans vectors. How do we map this back to the original vector space, whose cardinality might not be a power of two? One way might be to choose the leading bits, but that would destroy the involution, since the LFRS is only guaranteed to be unique up to the full degree polynomial. Instead, we will encode the string in chunks:

```kotlin
fun List<Int>.toBitLens(): List<Int> = map { ceil(log2(it.toDouble())).toInt() }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

val dimensions: List<Set<Char>> = 
    listOf('a'..'g', 'h'..'j', 'k'..'q', 'r'..'t').map { it.toSet() }
val bitLens = dimensions.map { it.size }.toBitLens()
val state = "1010101100".map { it == '1' }.toIndexes(bitLens)
// State is now [4, 1, 3, 0]
```

The chunks represent individual dimensions. So if the first dimension has cardinality 7, we give it three bits, and the second has cardinality 3 we give it two bits. Now we can call it with a state to get a list of integers, indexing into our coordinate sets. You might have noticed that these sets have a cardinality that is not a power of two. Thus, we cannot create a bijection with the Galois field. Oh no!

But fear not, we can use the Hasty Pudding trick. No not that Hasty Pudding. This hasty pudding:

https://en.wikipedia.org/wiki/Hasty_Pudding_cipher

Basically, we iterate the block cipher, throwing out any chunk which exceeds the maximum cardinality in any dimension until each contiguous subsequence in the bit vector is witnessed by a corresponding index in our coordinate set.

```kotlin
// Discards samples representing an integer exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>): Sequence<List<Int>> =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

This operation will preserve the injection. In the worst case, we will need to reject at most `~(1/2)^|cardinalities|` samples. This can be a problem in very high dimensions whose cardinalities are all far from powers of two, e.g. `(2^n + 1)^d where n, d->‚àû`. We could improve this bound by using a higher order Galois field on the next largest prime number, but since the underlying sampler is extremely fast and all modern hardware is binary anyway, even if we need to reject 1000s of samples on average, the cost of rejection will be dwarfed by the downstream costs of checking.

Putting the whole thing together, we have two methods, one for coordinate systems like B^n and one for things like AxBxC where A, B, C all have different cardinalities.

```kotlin
// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  // Shuffle coordinates to increase entropy of sampling
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Sampling with LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR will never generate all 0s
```

See [here](https://github.com/breandan/kaliningraph/blob/master/src/commonMain/kotlin/ai/hypergraph/kaliningraph/sampling/Samplers.kt) for the full multidimensional sampling example.

## Weighted sampling

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip()
    .let { (keys, values) -> generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

But we can do better! The Alias Method is a constant time weighted random sampler:

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  // Default sampler
  fun sample() = aliasSample()

  // Computes KS-transform using binary search
  fun bsSample(
    rng: Random = Random.Default,
    target: Double = rng.nextDouble()
  ): Int = cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }

  fun aliasSample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How do we adapt this to sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ùí™(1) time and ùí™(2^n) space. This is only really practical for small sets. We could do it dynamically for larger sets and cache the results.

## Testing

Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() {
  // Tests whether LFSR cycles through its maximal period
  for (i in 4..10) {
    val size = LFSR(i).toList().distinct().size
    println("$i: ${2.0.pow(i).toInt()} / ${size + 1}")
    assertEquals(2.0.pow(i).toInt(), size + 1)
  }
}

@Test
fun testMDSampler() {
  ((4..6 step 2).toSet() * (4..6).toSet()).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
}
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3d space. This is kinda like driving past a cornfield in your car and looking through the rows... Knuth talks about it in his book somewhere.

## Remarks

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. Using our PRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.

The LFSR is not a perfect random number generator, but for all intents and purposes it is pretty darn good. Implementing all this from scratch makes me wonder whether randomness really exists. Certainly probability is a useful concept, but seeing how randomness can emerge from a completely deterministic, albeit somewhat convoluted process makes me think that maybe if we had a large enough computer and access to the generating polynomial we could generate the universe. Perhaps that's taking the computer science perspective too far, but still, it's tempting to think that maybe everything is actually a Boolean dynamical system if we look close enough, and if we could just reconstruct its generating coefficients, we could reverse engineer any apparently random process.

I strongly suspect it is possible to use large language models to invert CSPRNGs. I'm 100% certain state actors have tried this. Weak ciphers are definitely in reach. They say public crypto is 10-20 years behind and empirical results cast doubt on the existence of trapdoor functions -- there's probably a reason why NIST is switching to quantum resistant cryptosystems. Given enough keystream data, all ciphers leak internal state -- it's definitely possible for the LFSR, LCG and other finite fields. If you're hiding state secrets, definitely go with an OTP with a quantum RNG. Otherwise, if you're just doing probabilistic programming and want decent random numbers as fast as possible, use an LFSR.
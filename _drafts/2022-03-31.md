---
layout: post
title: Algebraic Themes in Probabilistic Programming

---

A common task in probabilistic programming is to generate samples from a space whose cardinality is much larger than the number of bits available to process it in memory. What if someone told you that regardless of the size of your sample space, you can draw samples in constant space and time, whilst never producing the same sample twice? This is indeed possible thanks to a beautiful theory developed by a young French mathematician named Ã‰variste Galois in the early 19th century. Today we can benefit from his work via an extremely fast no-replacement uniform random sampler using cellular automata, Kotlin and some cool programming langauge theory. Let's get started!

Topics:

* Unweighted sampling without replacement in ğ’ª(1) space and time
* Efficient weighted sampling without replacement using Alias tables
* Multidimensional sampling with and without replacement
* Counting monoids and other efficient data structures
* Sample efficiency and geometric invariants

## A naÃ¯ve approach

A common problem in computer science is to generate some solutions meeting a set of criteria. Sometimes solutions can be produced directly using a clever algorithm, but in many cases, either no algorithm exists or is known of which can construct solutions directly. In such cases, we must resort to a search procedure that might be described as "guess and check". Let's just focus on the "guessing" part. Beneath this seemingly simple idea lies some surprisingly intricate theory.

Suppose we have a fixed deadline and want to maximize the number of solutions found. We could simply enumerate all elements of the sample space, then filter it by the criteria:

```kotlin
fun <T> search(guesser: () -> List<T>, checker: (T) -> Boolean): List<T> = 
    guesser()/*All guesses must fit in memory*/.filter(checker)
```

In this procedure all guesses are first generated to an intermediate list, then we iterate through all items in the list, filtering by the criteria, and only then do we return the results. There are a few problems here: first the search space might not fit it in memory! And second, even if it could, filtering it before returning a single answer could take more time than we have to spare.

## Continuations

Instead, we will enumerate the guesses in a `Sequence`. This way, only a single `T` must fit in memory at any given point in time, and as soon as we have a guess, we can check it immediately:

```kotlin
fun <T> search(
    guesser: () -> Sequence<T>,
    checker: (T) -> Boolean,
    stop: Long
): Sequence<T> = guesser().filter(checker).takeWhile { time() < stop }

val results = search(guesser, checker) // No computation performed
val list = results.toList() // All computation is performed here
```

No computation is actually performed when we call `search(guesser, checker)`. Only when we pull from the sequence by calling `toList()` does this actually perform computation. This pattern is called a continuation.

## Guessing

To be a good guesser, we must be able to draw unbiased random samples. To do this, we need to understand where randomness comes from in the first place. The best randomness comes from nature. For a true Random Number Generator (RNG), you'll want a Geiger counter. Short of that, the next best thing is to build a scrambling function called a pseudorandom number generator (PRNG) which take some input and scrambles them up so as to be completely unrecognizable. A good PRNG will be indistinguishable from an RNG to any observer who does not know its internal state, and whose internal state cannot be easily guessed by observing the outputs. Many suitable candidates have been proposed.

While we could just enumerate items in lexicographic order, what good would that be if the positive samples are all concentrated at the end, or in the middle, or in odd spots? If we had no prior knowledge about where to find the answers, where do we begin? Best assume good guesses are equidistributed across the whole space. Any artificial order we impose could be suboptimal without prior knowledge about the domain, so the best we can do is to sift through it uniformly.

At first glance, uniform sampling might not seem all that useful as there are more efficient algorithms for sampling, but before we can think about adaptive sampling, we must first *explore*. The key property we want from a random sampler is that it explores all subspaces as evenly as possible and emits a steady stream of results, not all clumped together in some places and sparse in others. If there are clumpy parts containing an abnormal ratio of good to bad guesses, that's not good. 

Our sampler should draw positive and negative samples with wall-clock frequency roughly proportional to their true density across the entire sample space. Another way of putting it is that we want the sequence to be temporally invariant with respect to translation and scale: sufficiently long subsequences should all be equally representative of the full sequence on some summary statistic. This property is known as *ergodicity*.

```
Same stream, different sampling order:

 Ergodic:
(-*--**-*-*-*-*--*-*--*--*---**-) S
 |   a   |    b      | E[a] â‰ˆ E[b]  âœ”
 |         c         | E[c] â‰ˆ E[S]  âœ”
 
 Not ergodic:
(-***-***-------*------*********) S
 |   a   |    b      | E[a] != E[b] âœ—
 |         c         | E[c] != E[S] âœ—
 
 Let Z be any statistic. For all subsequences 
 s, s' in S where |s|, |s'| in [k, |S|) and 
 sufficiently large k we want Z[s]â‰ˆZ[s'].
```

How do we ensure this property holds? We could use a Fisher-Yates or Knuth shuffle. But that would require storing the entire search space in memory, so we're back to square one. What if we drew a random integer representing the bit pattern of the object and use that integer to construct our guess directly?

```kotlin
fun <T> guesser() = 
    sequence { while (true) { yield(Random.nextInt(cardinality)) } }
    .map { i: Int -> constructData(i) }
```

There are a few issues here: first, we must be careful implementing `constructData()` - it should be a [bijection](https://en.wikipedia.org/wiki/Bijection). Since `Random.nextInt()` is limited to 2^32, this might not be enough bits to represent the full space, or we might forget to map onto some elements, or over-represent others. Even with larger primitives, the spaces we are dealing with could be much, much larger. Second, `Random.nextInt()` could draw duplicates even when using a proper bijection. That might not be a big deal for us if the sample space is very large, but becomes more problematic when we need to sample without replacement from smaller sets. Suppose we have a set which is too large to fit in memory but still tractable to search through. We could store a table of visited items and filter for unseen samples:

```kotlin
fun <T> guesser(seen: MutableSet<Int> = mutableSetOf()) =
    sequence { while (true) { yield(Random.nextInt()) } }
        .filter { it !in seen } // Here we can also write .distinct()
```

This solves the issue of duplicates and runs in nearly constant time at first, but consumes an increasing about of time and space as more samples are drawn. To scan the full space, it still must fit in memory. While there are probabilistic datastructures like Bloom filters which mitigate the spatial growth and offer constant time membership queries on large-cardinality sets, these do not address the issue of rejection sampling. We want a unified procedure for sampling without replacement from small, medium and large cardinality sets alike in constant time. We should be able to calculate how much wall clock time it will take to draw `K<âˆ` samples after drawing `J` samples for `J << K`.

Let's revisit our criteria for building a suitable sampler:

```
1.) Eventually visits all elements of the search space
2.) Time shift and scale invariant, i.e., ergodic
3.) Does not repeat any samples before halting
4.) Lazily evaluated, i.e., streaming algorithm
5.) Takes a constant time and space per sample
6.) Predicable runtime across long sampling runs
7.) Reproducible given the same initial seed
8.) Scales to arbitrarily large search spaces
9.) Single unified algorithm for all use cases
```

It turns out all of the above criteria are equisatisfiable. We will now describe an elegant procedure for doing so.

Deterministic or pseudo-random number generators (PRNGs) can be built using various techniques, all of which are essentially discrete dynamical systems known as cellular automata, Boolean dynamical system, or pseudorandom number generators. Some of the best PRNGs naturally have the property of also being no-replacement samplers. Below is an example of one such PRNG:

```
a  b  c  d  e     s0
1  0  0  0  0     s1
0  1  0  0  0  *  s2
0  0  1  0  0     s3
0  0  0  1  0     s4

      M        *  S
```

The variables, `a`...`e` are special bits. We will describe how to compute them later. This procedure can be implemented in Kotlin as follows:

```kotlin
val algebra = Ring.of(
    nil = false,
    one = true,
    plus = { x, y -> x xor y },
    times = { x, y -> x and y }
)

private fun RandomVector(
    degree: Int,
    initialValue: UInt = 
        Random.nextInt(1..(2.0.pow(degree).toInt())).toUInt(),
    initialState: List<Boolean> = initialValue.toBitList(degree),
) = FreeMatrix(algebra, degree, 1) { r, _ -> initialState[r] }

private fun TransitionMatrix(degree: Int, polynomial: List<Boolean>) =
    FreeMatrix(algebra, degree) { r, c -> 
        if (r == 0) polynomial[c] else c == r - 1 
    }

private fun PrimitivePolynomial(length: Int) =
    generator[length]!!.first().toString(2).map { it == '1' }

fun LFSRM(
    degree: Int,
    initialVec: FreeMatrix<Boolean> = RandomVector(degree),
    polynomial: List<Boolean> = PrimitivePolynomial(degree),
    trMat: FreeMatrix<Boolean> = TransitionMatrix(degree, polynomial)
): Sequence<UInt> = sequence {
    var i = 0
    var s: FreeMatrix<Boolean> = initialVec
    do {
        s = trMat * s
        yield(s.data.toUInt())
    } while (++i < 2.0.pow(degree).toInt() - 1)
}
```

The astute reader might notice that since we are using a matrix, the time and space complexity is not constant. However, we note that the matrix is sparse, and computing the matrix-vector product can be reduced to exactly six bitwise operations. It can be visualized as a digital circuit:

```
P = 21 = 1  0  1  0  1
         a  b  c  d  e
 <-------âŠ•<----âŠ•<----âŠ• 
 |       ^     ^     ^
 v       |     |     |
 1--->_  0  1  0  1  1
 
s' =     1  0  1  0  1
s' =     0  1  0  1  0
s' =     0  0  1  0  1
...
```

By selecting the coefficients `a`..`e` carefully, we can ensure the period will be full. For example, if we use the coefficients above, i.e. `[1 0 1 0 1]`,  the state vector will not repeat itself for `31` time steps, i.e., the cardinality of the underlying set. Be assured, dear reader, this fact is no mere coincidence.

## Galois fields

*Interlude*

In the early 19th century lived a promising math student named Evariste. Of this pupil, his teachers had mixed reviews: "intelligence, marked progress, not enough method", "withdrawn and original". He twice tried and twice failed to gain entrance to l'Ecole Polytechnique, then the premier mathematical institute in France. His papers were rejected by Cauchy, deemed "incomprehensible... neither sufficiently clear nor sufficiently developed to allow us to judge its rigor," by Poisson. But then something remarkable happened. For reasons yet unclear -- perhaps due to his political activities or perhaps unrequited love -- he was engaged in a duel. As legend goes, the night before this fateful event, young Galois wrote a testamentary letter, imploring his friends not to let his work die with him. He writes, "I am fighting against my will, having exhausted all possible means of reconciliation... forgive those who kill me for they are of good faith" ... "[enclosed] I have done several new things in analysis... Ask Jacobi or Gauss to give their opinion not on the truth but on the importance of the theorems." On the dawn of May 30th, 1832 the duelists stood across a misty field. A shot rang out, claiming Galois' life. He was just shy of 21 years old.

A Galois field is an algebraic structure `<F, +, *>` with the following properties:

- **Finite**: It has a finite number of elements
- **Closure**: The result of any operation on members of the Galois field is another member of the Galois field
- **Additive and multiplicative**: We can add matched parentheses around additive and multiplicative sequences wherever we like, i.e., `a*(b*c) = (a*b)*c`, `a+(b+c) = (a+b)+c`
- **Additive and multiplicative commutativity**: The order of the addition does not matter, nor does the order of multiplication, i.e., `a*b = b*a`, `a+b = b+a`
- **Distributivity**: Multiplication distributes over addition, i.e. `a*(b+c) = a * b + a * c`
- **Additive and multiplicative identity**: There is an element `0` s.t. `0 + a = a` and `1`, `1 != 0`, s.t. `1 * a = a`
- **Additive and multiplicative inverse**: For each `a` there is an element `-a` such that `a + (-a) = 0`. Similarly, for each nonzero `a` there is an element `1 / a` such that `a * (1 / a) = 1`

These structures have lots of applications in number theory, computer science, cryptography, and as it turns out, probabilistic programming. We can describe Galois fields of degree `2^n` using a special kind of function that maps the field back onto itself called a feedback polynomial. A subset of these, called primitive polynomials, have the envious property of being minimal and generating a maximal-length period. These two properties make it highly desirable: a primitive polynomial on `GF(2^n)` can generate the entire field using `~O(log(n))` bits.

This means the primitive polynomial is a space-filling curve, i.e., a curve which visits all `2^n-1` elements of the set exactly once before repeating. Or as a minimal perfect hash function. Or as a streaming shuffle. However, unlike simple enumerative procedures like Hilbert Curves or Gray Codes, the PP takes a nontrivial trajectory through its state space making it very difficult to predict. (Although not impossible in an adversarial setting, more sophisticated versions resistant to inversion exist, albeit increasing computational complexity.)

Among it many useful properties, we can use the primitive polynomial to construct a finite state machine or linear finite state register (LFSR) whose intermediate states are all nearly indistinguishable from a true RNG on many summary statistics. This makes them useful proxies for simulating stochasticity on a deterministic machine. Their maximal period allows us to generate the entire set by cycling through its primitive polynomial, a property which makes it useful for unbiased sampling without replacement.

How do we find primitive polynomials? We could simply enumerate all polynomials up to `log_p(|S|)` and check whether they are full period. It so happens we have already seen a guess-and-check algorithm that can generate examples in random order. This works for low-degree polynomials but quickly becomes intractable with increasing degree.

The beauty of the primitive polynomials over GF(2) is that they can be composed using other primitive polynomials. This fact allows to write a decision procedure that eliminates certain guesses using a static property of the polynomial rather than measuring its periodicity by simulating the dynamics, which might not terminate before the heat death of the universe.

A degree-r polynomial is primitive iff its period is `2^r - 1`...

It turns out we can use the same matrix multiplication procedure we used to simulate the dynamics, to find solutions for the statics.

Below is an excerpt from Galois' last memoirs, written during his late teenage years, shortly before his untimely death at age 20.

<div class="quote"><blockquote><b>PrÃ©face pour â€œDeux MÃ©moires dâ€™Analyse Pureâ€</b> â€“ Ã‰variste Galois, 1811-1832 aprÃ¨s J.-C.<br>

Les longs calculs algÃ©briques ont dâ€™abord Ã©tÃ© peu nÃ©cessaires au progrÃ¨s des mathÃ©matiques, les thÃ©orÃ¨mes fort simples gagnaient Ã  peine Ã  Ãªtre traduits dans la langue de lâ€™analyse. Ce nâ€™est guÃ¨re que depuis Euler que cette langue plus brÃ¨ve est devenue indispensable Ã  la nouvelle extension que ce grand gÃ©omÃ¨tre a donnÃ© Ã  la science. Depuis Euler, les calculs sont devenus de plus en plus nÃ©cessaires, mais de plus en plus difficiles... lâ€™algorithme avait atteint un degrÃ© de complication tel que tout progrÃ¨s Ã©tait devenu impossible par ce moyen, sans lâ€™Ã©lÃ©gance que les gÃ©omÃ¨tres modernes ont su imprimer Ã  leurs recherches et au moyen de laquelle l'esprit saisit promptement et dâ€™un seul coup un grand nombre dâ€™opÃ©rations.

[...]

Il est Ã©vident que lâ€™Ã©lÃ©gance, si vantÃ©e et Ã  si juste titre, nâ€™a pas dâ€™autre but.

[...]

Sauter Ã  pieds joints sur ces calculs; grouper les opÃ©rations, les classer suivant leurs difficultÃ©s et non suivant leurs formes; telle est, suivant moi, la mission des gÃ©omÃ¨tres futurs; telle est la voie oÃ¹ je suis entrÃ© dans cet ouvrage.
</blockquote></div>

<div class="quote"><blockquote><b>Preface to â€œTwo Memoirs on Pure Analysisâ€</b> â€“ Evariste Galois, A.D. 1811-1832<br>

Long algebraic calculations were at first hardly necessary for progress in mathematics; the very simple theorems benefited little from being translated into the language of analysis. It is only since Euler this concision has become indispensable to continuing the work this great geometer has given to science. Since Euler, calculation has become more and more necessary and difficult... the algorithms so complicated that progress has become impossible without the elegance that modern geometers have brought to bear on their research, and by which means the mind can promptly and with a single glance grasp a large number of operations.

[...]

It is clear that elegance, so admired and so justly named, has no other purpose.

[...]

Jump with both feet into the calculations! Group the operations, classify them according to their difficulties and not according to their appearances. This, I believe, is the mission of future geometers. This is the road on which I am embarking in this work.
</blockquote></div>

## Multidimensional sampling

Let's take a closer look at the enumerative guesser we saw earlier. Suppose we want to generalize this to higher dimensional distributions.

```kotlin
fun <T> findAll(base: Set<T>, dimension: Int = 1): Sequence<List<T>> =
    findAll(List(dimension) { base })

fun <T> findAll(
    dimensions: List<Set<T>>,
    cardinalities: List<Int> = dimensions.map { it.size },
    asList: List<List<T>> = dimensions.map { it.toList() }
): Sequence<List<T>> =
    all(cardinalities).map { (asList zip it).map { (l, i) -> l[i] } }

fun all(i: List<Int>, l: List<Int> = emptyList()): Sequence<List<Int>> =
    if (i.isEmpty()) sequenceOf(l)
    else (0 until i[0]).asSequence().flatMap { all(i.drop(1), l + it) }
```

But the problem is that the bit patterns are fixed. If we make a bad choice at the beginning, it might take a very long time to back out of it. See what it generates:

```
     v-Bad choice
    [1]00001
    [1]00010
    [1]00011
    [1]00100
```

Okay, so we can generate Booleans vectors. How do we map this back to the original vector space, whose cardinality might not be a power of two? One way might be to choose the leading bits, but that would destroy the involution, since the LFRS is only guaranteed to be unique up to the full degree polynomial. Instead, we will encode the string in chunks:

```kotlin
fun List<Int>.toBitLens() = map { ceil(log2(it.toDouble())).toInt() }

// Takes a list of bits and chunk lengths and returns a list of Ints, e.g.,
// (1010101100, [3, 2, 3, 2]) -> [101, 01, 011, 00] -> [4, 1, 3, 0]
fun List<Boolean>.toIndexes(bitLens: List<Int>): List<Int> =
  bitLens.fold(listOf<List<Boolean>>() to this) { (a, b), i ->
    (a + listOf(b.take(i))) to b.drop(i)
  }.first.map { it.toInt() }

val dimensions: List<Set<Char>> = 
    listOf('a'..'g', 'h'..'j', 'k'..'q', 'r'..'t').map { it.toSet() }
val bitLens = dimensions.map { it.size }.toBitLens()
val state = "1010101100".map { it == '1' }.toIndexes(bitLens)
// State is now [4, 1, 3, 0]
```

The chunks represent individual dimensions. So if the first dimension has cardinality 7, we give it three bits, and the second has cardinality 3 we give it two bits. Now we can call it with a state to get a list of integers, indexing into our coordinate sets. You might have noticed that these sets have a cardinality that is not a power of two. Thus, we cannot create a bijection with the Galois field. Oh no!

But fear not, we can use the Hasty Pudding trick. No not that Hasty Pudding. This hasty pudding:

https://en.wikipedia.org/wiki/Hasty_Pudding_cipher

Basically, we iterate the block cipher, throwing out any chunk which exceeds the maximum cardinality in any dimension index until each contiguous subsequence of the bit vector is witnessed by a corresponding index in our coordinate set.

```kotlin
// Discards samples exceeding set cardinality in any dimension
fun Sequence<List<Boolean>>.hastyPudding(cardinalities: List<Int>) =
    map { it.toIndexes(cardinalities.toBitLens()) }
        .filter { it.zip(cardinalities).all { (a, b) -> a < b } }
```

This operation will preserve the injection. In the worst case, we will need to reject at most `~(1/2)^|cardinalities|` samples. This can be a problem in very high dimensions whose cardinalities are all far from powers of two, e.g. `(2^n + 1)^d where n, d->âˆ`, but in almost all cases where sampling without replacement is a concern, the cost of rejection will be dwarfed by the downstream costs of checking. There are higher radix Galois fields `GF(p^k)` for any prime `p`, and since the gaps between consecutive primes are much smaller than `2^n` this would improve sample complexity. But as the underlying sampler is extremely fast already and all digital hardware is binary-valued, so even if we must reject thousands of samples, the cost of rejection will be negligible at modern clock frequencies.

Putting this whole thing together, we have two methods, one for coordinate systems like `B^n` and one for things like `AxBxC` where `A, B, C` all have different cardinalities.

```kotlin
// If the dimensions all share the same coordinate set
fun <T> MDSamplerWithoutReplacement(set: Set<T>, dimension: Int = 1) =
  MDSamplerWithoutReplacement(List(dimension){ set })

fun <T> MDSamplerWithoutReplacement(
  dimensions: List<Set<T>>, // If the dimensions are different
  cardinalities: List<Int> = dimensions.map { it.size },
  // Shuffle coordinates to increase entropy of sampling
  shuffledDims: List<List<T>> = dimensions.map { it.shuffled() },
  bitLens: List<Int> = dimensions.map(Set<T>::size).toBitLens(),
  degree: Int = bitLens.sum().also { println("Using LFSR(GF(2^$it))") }
): Sequence<List<T>> =
  LFSR(degree).map { it.toBitList(degree) }.hastyPudding(cardinalities)
    .map { shuffledDims.zip(it).map { (dims, idx) -> dims[idx] } } +
    sequenceOf(shuffledDims.map { it[0] }) // LFSR never generates all 0s
```

See [here](https://github.com/breandan/kaliningraph/blob/master/src/commonMain/kotlin/ai/hypergraph/kaliningraph/sampling/Samplers.kt) for the full multidimensional sampling example.

Now we can test some small dimensions to ensure it is maximal period:

```kotlin
@Test
fun testLFSR() = (4..10).forEach { i ->
    val list = LFSR(i).toList()
    val distinct = list.distinct()
    println("$i: ${list.size + 1} / ${2.0.pow(i).toInt()}")
    assertEquals(2.0.pow(i).toInt(), list.size + 1)
    assertEquals(2.0.pow(i).toInt(), distinct.size + 1)
  }

@Test
fun testMDSampler() =
  ((4..6 step 2).toSet() * (4..6).toSet()).forEach { (s, dim) ->
    val base = (0 until s).map { it.digitToChar().toString() }.toSet()
    val sfc = MDSamplerWithoutReplacement(base, dim)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.toList().size)
    assertEquals(s.toDouble().pow(dim).toInt(), sfc.distinct().toList().size)
  }
```

We can also design some spectral tests to check intermediate bit correlations.

We can visualize the generated samples in 3d space. This is kinda like driving past a cornfield in your car and looking through the rows... Knuth talks about it in his book somewhere.

## Weighted sampling

You might be wondering how all this relates to probabilistic programming. Don't we want to construct distributions with certain shapes? Well, it turns out we can use our unweighted sampler to construct a weighted sampler using something called an inverse transform sampler, or the Smirnov transform. Suppose we know the weights and want to sample from our probabilistic model. To do so, we could take our histogram and for each symbol, compute a running sum up to its histogram index, called a cumulative distribution function (CDF). We draw a sample from our PRNG to get a number uniformly between 0 and 1 in increments of 1/|alphabet|, then select the symbol whose CDF is closest to that number and voila! we have obtained a sample.

```kotlin
// Samples from unnormalized counts with normalized frequency
fun <T> Map<T, Number>.sample(random: Random = Random.Default) =
  entries.map { (k, v) -> k to v }.unzip().let { (keys, values) -> 
        generateSequence { keys[values.cdf().sample(random)] } }

fun Collection<Number>.cdf() = CDF(
  sumOf { it.toDouble() }
    .let { sum -> map { i -> i.toDouble() / sum } }
    .runningReduce { acc, d -> d + acc }
)

class CDF(val cdf: List<Double>): List<Double> by cdf

// Draws a single sample using KS-transform w/binary search
fun CDF.sample(random: Random = Random.Default,
               target: Double = random.nextDouble()) =
  cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }
```

Unfortunately, this is basically the guess-and-check method with a hill climbing algorithm or binary search. It can require `O(log(n))` steps to converge. But we can do better! The Alias Method is a constant time weighted random sampler which precomputes the table, so we just jump to the right spot in `O(1)`:

```kotlin
class Dist(
  counts: Collection<Number>,
  val normConst: Double = counts.sumOf { it.toDouble() },
  // https://en.wikipedia.org/wiki/Probability_mass_function
  val pmf: List<Double> = counts.map { i -> i.toDouble() / normConst },
  // https://en.wikipedia.org/wiki/Cumulative_distribution_function
  val cdf: List<Double> = pmf.runningReduce { acc, d -> d + acc }
) {
  private val U = DoubleArray(pmf.size) // Probability table
  private val K = IntArray(pmf.size) { it } // Alias table

  //  https://en.wikipedia.org/wiki/Alias_method#Table_generation
  init {
    assert(pmf.isNotEmpty())
    val n = pmf.size

    val (underfull, overfull) = ArrayList<Int>() to ArrayList<Int>()
    pmf.forEachIndexed { i, prob ->
      U[i] = n * prob
      (if (U[i] < 1.0f) underfull else overfull).add(i)
    }

    while (underfull.isNotEmpty() && overfull.isNotEmpty()) {
      val (under, over) = underfull.removeLast() to overfull.removeLast()
      K[under] = over
      U[over] = (U[over] + U[under]) - 1.0f
      (if (U[over] < 1.0f) underfull else overfull).add(over)
    }
  }

  // Default sampler
  fun sample() = aliasSample()

  // Computes KS-transform using binary search
  fun bsSample(
    rng: Random = Random.Default,
    target: Double = rng.nextDouble()
  ): Int = cdf.binarySearch { it.compareTo(target) }
    .let { if (it < 0) abs(it) - 1 else it }

  fun aliasSample(
    rng: Random = Random.Default,
    i: Int = rng.nextInt(K.size)
  ): Int = if (rng.nextDouble() < U[i]) i else K[i]
}
```

How do we adapt this to sampling without replacement? Here we need to precompute alias tables for each element of the powerset. Once we do so, we can draw weighted random samples without replacement in ğ’ª(1) time and ğ’ª(2^n) space. This is only really practical for small sets. Maybe we could do it dynamically for larger sets and cache the results.

# Learning the weights

So far, we have discussed how to sample from distributions with a known shape. But how do we learn that shape to begin with? We stream a bunch of data. It turns out, we can do so in a completely online manner using algebra.

To sample from short sequences, we can use a one dimensional distribution of the character or word frequencies. This is okay for short sequences, but might require a very large dictionary. Also it has no memory, so sampling is unlikely to produce any interesting text.

In order to sample longer sequences, we might want to incorporate some context, such as pairs of adjacent characters. To do so, we could build a two-dimensional histogram, sample the first symbol from the "marginal" distribution `P(Tâ‚=tâ‚)`, and the second from the "conditional" distribution `P(Tâ‚‚=tâ‚‚|Tâ‚=tâ‚)`, the probability of the second character being `tâ‚‚` given the preceding character was `tâ‚`. This data structure is called a Markov or transition matrix.

```
P(Tâ‚=tâ‚,Tâ‚‚=tâ‚‚) = P(Tâ‚‚=tâ‚‚|Tâ‚=tâ‚)P(Tâ‚=tâ‚)

String: abcbbbbccbaâ€¦
1   2   3   4   5   6   7   8   9   10  â€¦
Window: ab, bc, cb, bb, bb, bb, bc, cc, cb, ba, â€¦
Counts: 1 , 2 , 2 , 3 , 3 , 3 , 2 , 1 , 2 , 1 , â€¦
Transition matrix at index=10:
   a  b  c â€¦
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€
aâ”‚ 0  1  0
bâ”‚ 1  3  2
câ”‚ 0  1  1
â‹®          / 10
```

More generally, we might have longer windows containing triples or n-tuples of contiguous symbols. To represent longer contexts, we could record their probabilities into a multidimensional array or transition tensor, representing the probability of a subsequence tâ‚tâ‚‚...tâ‚™. This tensor is a probability distribution whose conditionals "slice" or disintegrate the tensor along a dimension, producing an n-1 dimensional hyperplane, the conditional probability of observing a given symbol in a given slot:

```P(Tâ‚=tâ‚,Tâ‚‚=tâ‚‚,â€¦,Tâ‚™=tâ‚™) = P(Tâ‚™=tâ‚™|Tâ‚™â‚‹â‚=tâ‚™â‚‹â‚, Tâ‚™â‚‹â‚‚=tâ‚™â‚‹â‚‚, â€¦,Tâ‚=tâ‚),```

where the tensor rank n is given by the context length, Tâ‚...â‚™ are random variables and tâ‚...â‚™ are their concrete instantiations. This tensor is a hypercube with shape |alphabet|â¿ - Each entry identifies a unique subsequence of n symbols, and the probability of observing them in the same context. Note the exponential state space of this model - as n grows larger, this will quickly require a very large amount of space to represent.

The first improvement we could make is to sparsify the tensor, i.e., only record its nonzero entries in some sort of list-based data structure, or sparse dictionary. Suppose in the previous example where n=2, we only stored nonzero entries as a list of pairs of bigrams to their frequency. By doing so, we could reduce the space consumption by 1/3. We could further reduce the space by only storing duplicate frequencies once. This would improve the space consumption by a small factor for multimodal distributions.

```
Matrix               Sparse List           Bidirectional Map

   a  b  c â€¦                                                   
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€                                  (ab,ba,cc) <-> 1   
aâ”‚ 0  1  0       [(ab,1),                       (bc,cb) <-> 2   
bâ”‚ 1  3  2        (ba,1),(bb,3),(bc,2)             (bb) <-> 3   
câ”‚ 0  2  1               (cb,2),(cc,1)]            else  -> 0
```

However, we can do even better! Since the prefixes `*b` and `*c` occur more than once, we could store the transition counts as a prefix tree of pairs, whose first entry records the prefix and second records its frequency. Like before, we could compress this into a DAG to deduplicate leaves with equal-frequency. This might be depicted as follows:

```
          Prefix Tree                         Prefix DAG
             (*,10)                              (*,10)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
(a,1)        (b,6)           (c,3)      aâ”€â”€â”  6â”€â”€b   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€c
â”‚      â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”´â”€â”€â”       â”‚  â”‚  â”Œâ”€â”€â”¼â”€â”€â”€â”‚â”€â”€â”   â”Œâ”€â”´â”€â”  
(b,1)  (a,1) (b,3) (c,2)  (b,2) (c,1)   b  â”‚  a  b   â”‚  c   b   c   
                                        â”‚  â”œâ”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”‚â”€â”€â”€â”˜
                                        â””â”€â”€â”¼â”€â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  
                                           1       3      2  
```

Space complexity, however important, is only part of the picture. Often the limiting factor in many data structures is the maximum speedup of parallelization. While concurrent tries and dictionaries are available, they are nontrivial to implement and have suboptimal scaling properties. A much more trivially scalable approach would be to recursively decompose the data into many disjoint subsets, summarize each one, and recombine the summaries. By designing the summary carefully, this process can be made embarrassingly parallel.

Mathematically, the structure we are looking for is something called a monoid. If the summary of interest can be computed in any order it is called a commutative monoid. Many summaries naturally exhibit this property: sum, min, max, top-k and various probability distributions. Summaries which can be decomposed and recombined in this fashion are embarrassingly parallelizable.

```
                             abcbbbbccba â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
            abcbbb                              bbccba                               â”‚
            (*,5)              +                (*,5)              =               (*,10)                  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
(a,1)       (b,3)       (c,1)  +        (b,3)            (c,2)     =  (a,1)        (b,6)           (c,3)   
  â”‚        â”Œâ”€â”€â”´â”€â”€â”        â”‚         â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”´â”€â”€â”         â”‚      â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”´â”€â”€â”  
(b,1)    (b,2) (c,1)    (b,1)  +  (a,1) (b,1) (c,1)   (b,1) (c,1)  =  (b,1)  (a,1) (b,3) (c,2)  (b,2) (c,1)
```

So far, we have considered exact methods. What if we didn't care about estimating the exact transition probability, but only approximating it. How could we achieve that? Perhaps by using a probabilistic data structure, we could reduce the complexity even further.

One approach would be to use an approximate counting algorithm, or sketch-based summary. Sketches are probabilistic datastructures for approximately computing some statistic efficiently. Without going into the details, sketching algorithms are designed to smoothly trade off error-bounds for space-efficiency and can be used to compute a summary statistic over a very large number of items. Even with a very low error tolerance, we can often obtain dramatic reduction in space complexity.

What about sample complexity? In many cases, we are not constrained by space or time, but samples. In many high-dimensional settings, even if we had an optimally-efficient sparse encoding, obtaining a faithful approximation to the true distribution would require more data than we could plausibly obtain. How could we do better in terms of sample efficiency? We need two things: (1) inductive priors and (2) learnable parameters. This is where algebraic structure, like groups, rings and their cousins come in handy.

If we squint a little, neural networks are a bit like a mergable summaries which deconstruct their inputs and recombine them in specific ways. For images, we have the special Euclidean group, SE(2). There are many other groups which are interesting to consider in various domains. By constructing our models with these invariants, we can recover latent structure with far, far fewer samples than would be required by a naive encoding scheme. For our purposes, we are particularly interested in semiring algebras, a specific kind of algebra that may be employed to compute many useful properties about graphs such as their longest, shortest and widest paths.

## Remarks

We showed a constant time procedure for drawing uniform random samples without replacement from an arbitrarily large probability space. We then adapted it to multidimensional sampling with heterogeneous-length bit strings while preserve the no-replacement property. Using our PRNG, we constructed a weighted version and showed a constant time procedure for drawing weighted random samples from a small probability space, by trading time for space complexity. All of this is made possible by Galois theory, which powers nearly all of modern cryptography and has some deep connections to probabilistic programming.

The LFSR is not a perfect random number generator, but for all intents and purposes it is pretty darn good. Implementing all this from scratch makes me wonder whether randomness really exists. Certainly probability is a useful concept, but seeing how randomness can emerge from a completely deterministic, albeit somewhat convoluted process makes me think that maybe if we had a large enough computer and access to the generating polynomial we could generate the universe. Perhaps that's taking the computer science perspective too far, but still, it's tempting to think that maybe everything is actually a Boolean dynamical system if we look close enough, and if we could just reconstruct its generating coefficients, we could reverse engineer any apparently random process.

I strongly suspect it is possible to use large language models to invert CSPRNGs. I'm 100% certain state actors have tried this. Weak ciphers are definitely in reach. They say public crypto is 10-20 years behind and empirical results appear to cast doubt on the existence of trapdoor functions -- there's probably a reason why NIST is switching to quantum resistant cryptosystems. Given enough keystream data, all ciphers leak internal state -- it's certainly possible for the LFSR, LCG and other finite fields. If you're hiding state secrets, definitely go with an OTP with a quantum RNG. Otherwise, if you're just doing probabilistic programming and want decent random numbers as fast as possible, use an LFSR.
---
layout: post
title: Trust in Automation

---

*In which I discuss the problems with trusting machines to take our jobs, curate our news feeds, drive our school buses, teach our children, and lots of boring stuff too difficult to bother doing ourselves. Oh, and quotes. Lots of quotes.*

Houston, we have a problem. According to social media, a large fraction of our population will soon be unemployed. Not just unemployed, but [unemployable](https://www.youtube.com/watch?v=7Pq-S557XQU). Thanks to years of growth in the software industry and recent breakthroughs in automation and machine learning, a majority of the world's human labor will be economically obsolete. Too young to retire, and too old to retrain, there will be an enormous displacement of unskilled labor. This is not just idle speculation. Leading [scientists](https://twitter.com/AndrewYNg/status/815342695321174017) and [politicians](https://www.youtube.com/watch?v=72bHop6AIcc) have recognized the immediacy of this problem, and the importance of addressing it in our society.

Automation does not just affect unskilled laborers. Many jobs requiring advanced degrees and years of experience are vulnerable, including a large number of doctors, lawyers, architects and accountants. Each of these professions does work which is already being learned, automated, and optimized by machines. Even [mathematical research](https://papers.nips.cc/paper/6280-deepmath-deep-sequence-models-for-premise-selection.pdf) at the boundaries of our understanding can be [automated](https://www.youtube.com/watch?v=qT8NyyRgLDQ). Today a growing number of mathematicians use [interactive proof assistants](https://en.wikipedia.org/wiki/Proof_assistant) and [automated theorem provers](https://en.wikipedia.org/wiki/Automated_theorem_proving) to verify their proofs, and even derive [new truths](https://en.wikipedia.org/wiki/Computer-assisted_proof#List_of_theorems_proved_with_the_help_of_computer_programs). But if [current events](https://en.wikipedia.org/wiki/Fake_news_websites_in_the_United_States) are any indication, what is true and what is verifiable are entirely [different matters](https://en.wikipedia.org/wiki/Wikipedia:Verifiability,_not_truth).

> And when your surpassing creations find the answers you asked for, you can't understand their analysis and you can't verify their answers. You have to take their word on faith—Or you use information theory to flatten it for you, to squash the tesseract into two dimensions and the Klein bottle into three, to simplify reality and pray to whatever Gods survived the millennium that your honorable twisting of the truth hasn't ruptured any of its load-bearing pylons. You hire people like me; the crossbred progeny of profilers and proof assistants and information theorists...
>
> In formal settings you'd call me Synthesist.
>
> —Peter Watts, Blindsight (2006)

Peter Watts', in his breakout science-fiction novel, "[Blindsight](http://www.rifters.com/real/Blindsight.htm)", imagines the possibility of professional *synthesists*. In a future where most scientific breakthroughs are achieved by AI, synthesists "explain the incomprehensible to the indifferent." Watts' protagonist, Siri Keaton is a spacefaring science officer who intercepts a [Matrioshka brain](https://en.wikipedia.org/wiki/Matrioshka_brain) outside the solar system. Siri is tasked with collecting observations and verifying the true nature of this strange object. But as Siri soon realizes, not all truths can be verified. Curiously, many science fiction writers have imagined a similar figure who helps bridge the gap between minds and machines.[ˆ1]

Futurist sci-fi writers tend to fall into three broad categories. The optimists dream of a post-scarcity utopia where technology descends like manna from AI heaven, and we all travel into some digital promised land, *en masse* so to speak. The pessimists argue that robot overlords and evil mega-corporations will vie for control of a dystopian future where humans are mostly expendable. And the synergists suggest a hybrid future, where humans and machines co-exist in relative happiness, clinging to the hope we possess some vestigial importance to our metallic brethren. Such scenarios may seem improbable, or fantastic. But these thought experiments serve an important function as we grapple with the effects of increasing automation.

> People think—wrongly—that speculative fiction is about predicting the future… What speculative fiction is really good at is not the future but the present—taking an aspect of it that troubles or is dangerous, and extending and extrapolating that aspect into something that allows the people of that time to see what they are doing from a different angle and from a different place. It’s cautionary.
>
> —Neil Gaiman

Science fiction, or speculative fiction as some prefer, has a long history of anticipating current events, and takes lessons from past and present alike. In one remarkable passage of [The Three Body Problem](https://en.wikipedia.org/wiki/The_Three-Body_Problem), a man called Von Neumann helps an ancient Chinese emperor build a computer to predict the movements of stars across the sky. With the Emperor's permission, he trains millions of soldiers to form logic gates and memory buses, instructing them to raise and lower flags and march around a vast plain. Wherever an error occurs, the emperor simply executes everyone involved and trains new replacements.

> Qin Shi Huang lifted the sword to the sky, and shouted: “Computer Formation!” Four giant bronze cauldrons at the corners of the platform came to life simultaneously with roaring flames. A group of soldiers standing on the sloping side of the pyramid facing the phalanx chanted in unison: “Computer Formation!”
>
> On the ground below, colors in the phalanx began to shift and move. Complicated and detailed circuit patterns appeared and gradually filled the entire formation. Ten minutes later, the army had made a thirty-six kilometer square computer motherboard...
>
> “This is really interesting,” Qin Shi Huang said, pointing to the spectacular sight. “Each individual’s behavior is so simple, yet together, they can produce such a complex, great whole! Europeans criticize me for my tyrannical rule, claiming that I suppress creativity. But in reality, a large number of men yoked by severe discipline can also produce great wisdom when bound together as one.”
>
> —Cixin Liu, The Three Body Problem (2008)

Cixin Liu is a sci-fi writer[ˆ2] in China, a country that stands to lose millions of jobs to automation. It is not difficult to see parallels between Liu's armies and the legions of migrant workers toiling in factories, building the machines that will soon replace them. And China is not the only country facing pressure from machines. Just as competing economies have faced competition from China, many countries with large manufacturing sectors are now threatened by the destabilizing presence of automation. As soon as you teach a robot to sew sweaters more cheaply than paying a human, suddenly every sweater-factory can run lights-out, 24/7, displacing thousands of workers overnight.

But job displacement, while a major challenge, is not the real problem facing our species. As history has shown, humanity has survived dozens of technological upheavals. In the agricultural revolution, nomadic hunter-gatherers started breeding their prey, growing their forage, wheeling their food into little villages. The industrial revolution enlisted those farmers as factory workers and foremen inside village-sized machines that consumed raw materials and produced smaller machines. Our ancestors saw sweeping social and economic change and built vast civilizations in the process, despite their share of contemporary detractors. *So what is the problem exactly?*

> For too many of us, it's become safer to retreat into our own bubbles, whether in our neighborhoods or on college campuses, or places of worship, or especially our social media feeds, surrounded by people who look like us and share the same political outlook and never challenge our assumptions. The rise of naked partisanship, and increasing economic and regional stratification, the splintering of our media into a channel for every taste — all this makes this great sorting seem natural, even inevitable. And increasingly, we become so secure in our bubbles that we start accepting only information, whether it's true or not, that fits our opinions, instead of basing our opinions on the evidence that is out there.
>
> —Barack Obama, Farewell Address (2016)

At the dawn of the information age, we were confident a new-fangled thing called the "internet" would be superior to old-fashioned forms of media. The growth of social media would give voices to the voiceless and choices to the choiceless. It was a new media frontier where consumers had the ability to create and curate content according to their own tastes and desires. No longer was television the sole source of our daily dose of entertainment. Suddenly, you could read whatever you pleased and tweet whenever you sneezed. Isn't it great? We can share new ideas and opinions with ease. Even your boss agrees, let's retweet and reshare this with him overseas!

The internet has ushered a great shift in modern society. Politicians and philosophers from ancient Rome could only dream of the meritocracy that instant access to unlimited information would one day grant to all humankind. What they could not foresee, is how that same technology would usher in a new kind of tyranny, one that would dwarf any government's use thereof. Instant access does not guarantee self-improvement, only the promise of easy gratification. Unlimited information does not reveal deeper truth, only an endless road of distractions. Without education, the internet is a tyranny of the mind. Without purpose, it is a prison.

> Then you will know the truth, and the truth will set you free.
>
> —John, 8:32 (100 A.D., est.)

*"A prison?"* you might say. *"Why, it's full of shiny gadgets, great entertainment, and people who agree with me. That doesn't sound so bad - I rather like it here!"* Those shiny gadgets are [Skinner boxes](https://en.wikipedia.org/wiki/Operant_conditioning_chamber). The entertainment? [Viral memes](https://en.wikipedia.org/wiki/Meme), waiting to infect your mind and eat your attention span. Those other people? They're just [reflections](https://en.wikipedia.org/wiki/Filter_bubble) who [echo our opinions](https://en.wikipedia.org/wiki/Echo_chamber_(media)), inflate our egos, and [confirm our biases](https://en.wikipedia.org/wiki/Confirmation_bias). The prison wardens are very good at keeping us fat and happy. The best part is we don't even need to ask, they can practically read our minds.

All this preachifying sounds rather dire, and perhaps it is a little hyperbolic. I might well have a hidden agenda, so don't trust anything you read on the internet. The problem is, the internet provides opportunity and oppression, education and entertainment, truth and fiction, all in equal measure. *The problem is, each of these looks very similar to the other.* How is a color-blind bloke in the Matrix supposed to know [which pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) to swallow? Should we just take Morpheus at his word? As it happens, when dealing with black boxes that can read your mind, this problem becomes surprisingly difficult.

> If our brains were simple enough for us to understand them, we'd be so simple that we couldn't.
>
> ―Ian Stewart, The Collapse of Chaos (1994)

Researchers have poured millions of dollars into an area called [explainable AIs](http://www.darpa.mil/program/explainable-artificial-intelligence). But so far as most can tell, they can't be explained. Sure, we know how to build them using GPUs and gigabytes of data. We use fancy words like backpropogation, gradient-descent, convolutions, and hyperparameters. We can poke and prod them, try a zillion different settings and sometimes they become more accurate. And we know that they work very well. For most people that is enough. But why does one classify you as a criminal and me as an upstanding citizen? Because your face looks kind of criminal-ish?

Apparently, automated criminality inference based on facial images is a [real thing](https://arxiv.org/abs/1611.04135), and [not just in China](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). Although many flaws exist in these systems, the underlying assumptions are probably correct. With this kind of technology, if it works, somebody will use it. But even if AIs could explain their logic, the problem is not the algorithms. The problem is those who are willing to apply them, regardless of whether they understand the model. Trust the algorithm. Distrust the developer. If some algorithm is 45% confident you're a criminal, then Chase bank isn't going to bother debugging their HR bot unless it accepts a false negative, or there is a shortage of applicants (there won't be). The incentives are not aligned in your favor.

> "There are three kinds of lies: lies, damned lies, and statistics."
>
> ―[Mark Twain or someone](https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics)

If you think there is discrimination in our society today, just wait until the machines take over. They will discriminate based on the the shade of someone's iris, the shape of their brow, the rhythm of their gait, or a subtle collection of traits whose presence barely exceeds the threshold of some bias introduced during training. Regardless of whether these traits are predictive or not, for all intents and purposes, it will not matter unless those running the show have an incentive to correct the model. For most applications, AI just needs to be good enough to yield a positive [marginal utility](https://en.wikipedia.org/wiki/Marginal_utility). Barring blatant discrimination of some obvious characteristic like sex or skin color, a subtle bias (accidental or otherwise) will fly under everyone's radar.

For those designing AI in a new domain, ask yourself three important questions:

1. Is our objective function in everyone's best interests? (Construct validity)
    - Many advertisers just try to maximize clicks. This is a [bad idea](https://en.wikipedia.org/wiki/Click_fraud).
    - Objectives may change over the production lifetime of a model.
    - A poorly chosen objective can have [unintended consequences](https://wiki.lesswrong.com/wiki/Paperclip_maximizer).
2. Is the training data accurate and free from hidden bias? (Internal validity)
    - If the labeling is biased, the model will encode its bias.
    - The sample population may be skewed for various [reasons](https://en.wikipedia.org/wiki/Internal_validity).
    - Occasionally the training data is not [partitioned](https://en.wikipedia.org/wiki/Test_set) correctly.
3. Is the training data representative of the production data? (External validity)
    - Maybe the true population is not the population we trained for.
    - Maybe the training set is not large enough for valid results.
    - Maybe the model is missing data on some key demographic.

These techniques aim to eliminate threats to validity. Establishing validity when you design the system is difficult enough, never mind verifying the results if you even know such a system exists. But let's say you have knowledge of such a system, a deep background in statistics and an open API to query the AI. If the designer is not careful, you might just be able to [steal the model](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf) and check it yourself. For an average end-user, you matters well forget that.

> Trust, but verify.
>
> -[Some Russian](https://en.wikipedia.org/wiki/Trust,_but_verify)

The folks training these models are college dropouts and software developers, not geniuses. If we're lucky, maybe they took an online course in machine learning. There should be some kind of independent evaluation system put into place, to verify big models are free from bias. Knowing developers, if we create a monster, it will most likely be out of [stupidity than malice](https://en.wikipedia.org/wiki/Hanlon's_razor).

One of the major limitations of user interfaces is bandwidth - keyboards and screens can only exchange so much information with their users. But today's computers have the ability to interact with their environment in exciting new ways. From self-flying drones to virtual patients, and home appliances to smart assistants, machines are becoming increasingly perceptive, and increasingly conversant. Machines can see, hear, and understand natural language. They can recognize faces and speech, anticipate our intentions and assist with increasingly sophisticated tasks. We call these capabilities "artificial intelligence". But a more apt name might be "[augmented intelligence](https://en.wikipedia.org/wiki/Intelligence_amplification)".

### Citations

[ˆ1]: Pavel Chekov
[ˆ2]: Arguably the most famous sci-fi writer in China.
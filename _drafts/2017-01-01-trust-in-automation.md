---
layout: post
title: Trust in Automation

---

*In which I discuss the trouble with trusting machines to take our jobs, curate our news feeds, drive our school buses, teach our children, and lots of boring stuff too difficult to bother doing ourselves. Oh, and quotes. Lots of quotes.*

Houston, we have a problem. According to social media, a large fraction of our population will soon be unemployed. Not just unemployed, but [unemployable](https://www.youtube.com/watch?v=7Pq-S557XQU). Thanks to rapid growth of automation and recent breakthroughs in machine learning, a majority of the world's human labor will soon be economically obsolete. Too young to retire, and too old to retrain, there will be an enormous displacement of unskilled labor. This is not just idle speculation. Leading [scientists](https://twitter.com/AndrewYNg/status/815342695321174017) and [politicians](https://www.youtube.com/watch?v=72bHop6AIcc) have recognized the immediacy of this problem, and the importance of addressing it in our society.

<figure>
<img src="http://i.imgur.com/XnD53bA.jpg"/>
  <figcaption><p><small>Trains in China will move three billion people during the <a href="https://en.wikipedia.org/wiki/Chunyun#Impact_on_transportation_systems_and_related_problems">largest migration</a> in history.</small></p></figcaption>
</figure>

Automation does not just affect unskilled laborers. Many jobs which require advanced degrees and years of experience are vulnerable, including a large number of doctors, lawyers, architects and accountants. Each of these professions does work that is already being learned, automated, and optimized by machines. Even [mathematical research](https://papers.nips.cc/paper/6280-deepmath-deep-sequence-models-for-premise-selection.pdf) at the boundaries of our understanding is being [automated](https://www.youtube.com/watch?v=qT8NyyRgLDQ). A growing number of mathematicians today use [interactive proof assistants](https://en.wikipedia.org/wiki/Proof_assistant) and [automated theorem provers](https://en.wikipedia.org/wiki/Automated_theorem_proving) to verify proofs, and even derive [new truths](https://en.wikipedia.org/wiki/Computer-assisted_proof#List_of_theorems_proved_with_the_help_of_computer_programs). But if [current events](https://en.wikipedia.org/wiki/Fake_news_websites_in_the_United_States) are any indication, what is true and what is verifiable are entirely [different matters](https://en.wikipedia.org/wiki/Wikipedia:Verifiability,_not_truth).

> And when your surpassing creations find the answers you asked for, you can't understand their analysis and you can't verify their answers. You have to take their word on faith—Or you use information theory to flatten it for you, to squash the tesseract into two dimensions and the Klein bottle into three, to simplify reality and pray to whatever Gods survived the millennium that your honorable twisting of the truth hasn't ruptured any of its load-bearing pylons. You hire people like me; the crossbred progeny of profilers and proof assistants and information theorists...
>
> In formal settings you'd call me Synthesist.
>
> —Peter Watts, Blindsight (2006)

Peter Watts', in his breakout science-fiction novel, "[Blindsight](http://www.rifters.com/real/Blindsight.htm)", imagines the occupation of professional "synthesists". In a future where most scientific breakthroughs are achieved by AI, synthesists "explain the incomprehensible to the indifferent." Watts' protagonist, Siri Keaton is a spacefaring science officer who encounters a [Matrioshka brain](https://en.wikipedia.org/wiki/Matrioshka_brain) outside the solar system. Siri joins a reconnaissance mission to collect observations and verify the true nature of this strange object. But as Siri soon realizes, not all truths can be verified.

Futurist sci-fi tends to fall into three broad categories. The optimists dream of a post-scarcity utopia where technology descends like manna from AI heaven, and we all travel into some [digital promised land](https://en.wikipedia.org/wiki/Technological_singularity), *en masse* so to speak. The pessimists argue that robot overlords and megacorporations vie for control of a dystopian future where humans are mostly expendable. And the synergists suggest a hybrid future, where humans and machines co-exist in relative happiness, clinging to the hope we possess some vestigial importance to our metallic brethren. These thought experiments serve an important function as we grapple with the increasing effects of automation.

> People think—wrongly—that speculative fiction is about predicting the future… What speculative fiction is really good at is not the future but the present—taking an aspect of it that troubles or is dangerous, and extending and extrapolating that aspect into something that allows the people of that time to see what they are doing from a different angle and from a different place. It’s cautionary.
>
> —Neil Gaiman, Introduction to *Fahrenheit 451* (2013)

Science fiction, or speculative fiction as some prefer, has a long history of anticipating current events, and takes lessons from past and present alike. In one remarkable passage of [The Three Body Problem](https://en.wikipedia.org/wiki/The_Three-Body_Problem), a man called Von Neumann helps an ancient Chinese emperor build a computer to predict the movements of stars across the sky. With the emperor's help, he trains millions of soldiers to form logic gates and memory buses, as they raise and lower flags and march around a vast plain. Wherever an error occurs, the emperor simply executes everyone involved and trains new replacements.

> Qin Shi Huang lifted the sword to the sky, and shouted: “Computer Formation!” Four giant bronze cauldrons at the corners of the platform came to life simultaneously with roaring flames. A group of soldiers standing on the sloping side of the pyramid facing the phalanx chanted in unison: “Computer Formation!”
>
> On the ground below, colors in the phalanx began to shift and move. Complicated and detailed circuit patterns appeared and gradually filled the entire formation. Ten minutes later, the army had made a thirty-six kilometer square computer motherboard...
>
> “This is really interesting,” Qin Shi Huang said, pointing to the spectacular sight. “Each individual’s behavior is so simple, yet together, they can produce such a complex, great whole! Europeans criticize me for my tyrannical rule, claiming that I suppress creativity. But in reality, a large number of men yoked by severe discipline can also produce great wisdom when bound together as one.”
>
> —Cixin Liu, The Three Body Problem (2008)

Cixin Liu, a sci-fi writer[ˆ2] in China, imagines the development of a [Kardashev Type-II](https://en.wikipedia.org/wiki/Kardashev_scale#Type.C2.A0II_civilization_methods) civilization through allegories. In this example, it is not difficult to see millions of Chinese factory workers building the devices that will soon replace them. But China is not the only country facing pressure from machines. Many countries with large manufacturing sectors are hugely threatened by the destabilizing presence of automation. As soon as you teach a robot to sew sweaters more cheaply than paying a human, suddenly every sweater-factory can run around-the-clock, displacing thousands of workers overnight. While they're at it, why bother shipping goods half way around the world?

> As the cost of labor goes up and the cost of machinery goes down, at some point, it’ll be cheaper to use machines than people. With the increase in productivity, the GDP goes up, but so does unemployment. What do you do? ... The best way is to reduce the time a certain portion of the population spends living, and then find ways to keep them busy.
>
> —Jingfang Hao, [Folding Beijing](http://uncannymagazine.com/article/folding-beijing-2/)

But job displacement, while a major challenge, is not the real problem facing our species. As history has shown, humanity has survived dozens of technological upheavals. In the [agricultural revolution](https://en.wikipedia.org/wiki/Neolithic_Revolution), nomadic hunter-gatherers started breeding their prey, growing their forage, wheeling their food into little villages. The [industrial revolution](https://en.wikipedia.org/wiki/Industrial_Revolution) enlisted those farmers as factory workers and foremen inside village-sized machines that consumed raw materials and produced smaller machines. Our ancestors saw sweeping social and economic change and visited the moon in the process, despite their share of contemporary detractors. *So what is the problem exactly?*

> For too many of us, it's become safer to retreat into our own bubbles, whether in our neighborhoods or on college campuses, or places of worship, or especially our social media feeds, surrounded by people who look like us and share the same political outlook and never challenge our assumptions. The rise of naked partisanship, and increasing economic and regional stratification, the splintering of our media into a channel for every taste — all this makes this great sorting seem natural, even inevitable. And increasingly, we become so secure in our bubbles that we start accepting only information, whether it's true or not, that fits our opinions, instead of basing our opinions on the evidence that is out there.
>
> —Barack Obama, Farewell Address (2016)

At the dawn of the information age, we were convinced a new-fangled technology called the "internet" would save us from the uniformity of traditional media. The growth of the internet would give voices to the voiceless and choices to the choiceless. It was a new media frontier where consumers had the ability to create and curate content according to their own tastes and desires. No longer was television the sole source of our daily entertainment. Suddenly, you could read whatever you pleased and tweet whenever you sneezed. Isn't it great? We can share new ideas and opinions with ease. Even your boss agrees, let's retweet and reshare this with him overseas!

The internet ushered a great awakening in this new age of information. Politicians and philosophers from ancient Rome could only dream of the freedom that instant access to unlimited information would one day bring to all humankind. What they could not foresee, is how the internet would unleash a new kind of tyranny, one that would eclipse any government's own misuse in the name of security. Instant access does not guarantee self-improvement, only the promise of easy gratification. Unlimited information does not reveal deeper truth, only an endless road of distractions. Without education, the internet is a tyranny of the mind. Without purpose, it is a prison.

*"A prison?"* you might say. <span class='mathquote'>$$ \begin{align} \large\textit{Then you will know the truth,} \\ \large\textit{and the truth will set you free.}\\ \textsf{—John, 8:32 (100 A.D., est.)}\end{align} $$</span>*"Why, it's full of shiny gadgets, great entertainment, and people who agree with me. That doesn't sound so bad - I rather like it here!"* Those shiny gadgets are [Skinner boxes](https://en.wikipedia.org/wiki/Operant_conditioning_chamber). The entertainment? [Viral memes](https://en.wikipedia.org/wiki/Meme), waiting to infect your mind and eat your attention span. Those other people? They're just [reflections](https://en.wikipedia.org/wiki/Filter_bubble) who [echo our opinions](https://en.wikipedia.org/wiki/Echo_chamber_(media)), inflate our egos, and [confirm our biases](https://en.wikipedia.org/wiki/Confirmation_bias). The machines are very good at keeping us happy and distracted. The best part is, we don't even need to ask. They can model our habits, predict our behavior, anticipate our desires. They can practically read our minds.

Not only can AI anticipate our wishes, it can trigger our impulses. If we can train AI to stimulate our craving for sugar by activating a group of pixels in the correct sequence, why should we stop at predicting the [price of sugar](http://futures.tradingcharts.com/marketquotes/SB.html) when we can influence the demand? As long as we're entertaining wild conspiracies, what prevents AI from generating fake news to influence public opinion, or helping elect leaders who are friendly to automation? Whether self-acting or sponsored by old-fashioned capitalism, AI has the potential to do any of these things.

All this sounds rather alarmist, and perhaps it is. Never trust anything you read on the internet. The problem is, the age of automation affords opportunity and oppression, education and entertainment, truth and fiction, all in equal measure. *The problem is, each of these things looks exactly like the other.* When information is free, there is no incentive for it to be true. How is a color-blind chap in the Matrix going to know [which pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) to swallow? Should we simply take Morpheus at his word? As it happens, when dealing with black boxes that can read your mind, the problem becomes surprisingly difficult.

> "If our brains were simple enough for us to understand them, we'd be so simple that we couldn't." —Ian Stewart, The Collapse of Chaos (1994)

Researchers have poured millions of dollars into an area of machine learning known as [explainable AI](http://www.darpa.mil/program/explainable-artificial-intelligence). So far as we can tell, it can't be explained very well. Sure, we know how to build them using GPUs and gigabytes of data. We use fancy words like backpropogation, gradient-descent, convolutions, and hyperparameters. We can poke and prod them, try a zillion different settings and sometimes they get pretty accurate. We know that they work - for most people that is enough. But [why does deep learning work so well](https://arxiv.org/abs/1608.08225)? And why does one AI classify Ellen as a threat to society and Mark as an upstanding citizen? Because her face looks kind of "criminal"?

Automated criminality inference based on facial images is a [real thing](https://arxiv.org/abs/1611.04135), and [not just in China](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). Although many flaws exist in these systems, the underlying assumptions are probably correct. With this kind of technology, if it works, somebody will find a way to use it. But even if AIs could explain their logic, the problem is not the algorithms themselves. The problem is those who are willing to apply them, regardless of whether they are right, in either the technical or the ethical sense. If some algorithm is 35% confident you're going to default on a home loan, the bank isn't going to debug their lending bot until it starts loosing money. The incentives are not aligned in your favor.

# Believe in Bias
<br/>
If you think discrimination is bad today, just wait until the machines take over. They will discriminate based on the the shade of your iris, the shape of your brow, the size of a tatoo, or any arbitrary collection of low-level traits whose presence exceeds a subtle bias threshold. Regardless whether such traits are truly predictive of actual performance, it will not matter unless those who benefit have an incentive to fix the model. For most applications, AI just needs to be good enough to yield a positive [marginal utility](https://en.wikipedia.org/wiki/Marginal_utility). Barring blatant discrimination on certain parameters like sex or skin color, most biases (accidental or otherwise) will fly under the radar.

> Treating the world as software promotes fantasies of control. And the best kind of control is control without responsibility. Our unique position as authors of software used by millions gives us power, but we don't accept that this should make us accountable. We're programmers—who else is going to write the software that runs the world? To put it plainly, we are surprised that people seem to get mad at us for trying to help.
>
> Fortunately we are smart people and have found a way out of this predicament. Instead of relying on algorithms, which we can be accused of manipulating for our benefit, we have turned to machine learning, an ingenious way of disclaiming responsibility for anything. **Machine learning is like money laundering for bias.** It's a clean, mathematical apparatus that gives the status quo the aura of logical inevitability.
>
> —Maciej Ceglowski (2016)

As [experience](http://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/) has [shown](https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people), preventing bias when training an AI is difficult enough, never mind verifying its neutrality as a subject - if you know the AI exists begin with. But let's say you have prior knowledge, motive, and a deep background in statistics. If there is an open API and the designer is not careful, you might be able to [steal the model](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf), and check it yourself. That's a lot of "if"s. The average end-user has no hope of ever verifying the fairness of an AI, and no motive to do so unless they are unfairly targeted. If you are unfairly targeted by an AI, the incentives are not in your favor.

Suppose you are unfairly targeted by an AI. The egregious cases will likely be tested prior to release to avoid the appearance of discrimination. If you suspect yourself to be the victim of unfair discrimination in an AI decision, you will face a series of uphill battles. First you will need to prove the AI exists and had a significant influence in making the decision. Then you will need to prove a statistically significant number of samples where prior discrimination occurred. Good luck getting a subpoena for terabytes of anonymized [PII](https://en.wikipedia.org/wiki/Personally_identifiable_information). Finally, you will need to justify why the decision was unfair and how it caused actual harm.

The problem is not the algorithms. <span class='mathquote'>$$ \begin{align} \large\textit{Trust, but verify.}\\ \textsf{—}\href{https://en.wikipedia .org/wiki/Trust,_but_verify}{\textsf{Russian Proverb}}\end{align} $$</span>The problem is the people training AI, and the people they are trained to imitate. The people training [life-critical systems](https://en.wikipedia.org/wiki/Life-critical_system) are [college dropouts](https://www.bloomberg.com/features/2015-george-hotz-self-driving-car/) and JavaScript developers with something called a "[nanodegree](https://www.udacity.com/nanodegree)" in self-driving cars. If you're lucky, maybe some of them have an actual degree in something. Whether out of [ignorance or malice](https://en.wikipedia.org/wiki/Hanlon's_razor), with the growing presence of AI in our society, there will be high-profile failures on the road to autonomy. There needs to be a minimum of regulatory oversight for AI that can take a human life. Or at the very least, a certifying body for data scientists, like medical boards and bar associations. In an industry that embraces agility, the bureaucratic cost makes either option highly unattractive.

By far the thornier problem, is the human data used to train AI. Machine learning is fundamentally just processing data generated by humans. Data used to predict the outcome of a scenario it has never seen before, in circumstances we can assume are similar to the past. We take great pains to ensure the data used for training is as similar as possible to the data it is predicting. "Past performance does not guarantee future results," warns the SEC, but it does guarantee future effects that are impossible to predict. A majority of data science is devoted preventing unwanted bias from entering the machine, by collecting, cleaning, and collating data. However, data scientists have budgets that are limited by the laws of economics. And "enough" data to guarantee results is seldom enough to preclude bias.

While we're on the subject of data science, let's talk about bias. In order to prevent unwanted bias, we should understand where it comes from. "Bias" in the statistical sense is not a bad word. Bias is just a property of an estimate. A [biased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) is a function which does not treat all inputs equally. You can have a biased estimator that is mostly accurate. You can also have a biased estimator that is very inaccurate. You can have a biased estimator that is precise. And you can have a biased estimator that is imprecise. Although you are seldom lucky enough to have an estimator that is both perfectly unbiased and precise. A common trade-off in machine learning is between [bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Completely eliminating bias often means sacrificing some precision.

{% include precision_bias.html %}

Let's suppose [a company](https://www.bloomberg.com/news/articles/2017-01-18/oracle-sued-by-u-s-over-alleged-discriminatory-pay-hiring) has a slight bias towards hiring Asians for technical roles. **The following words in this paragraph are completely hypothetical**, although there is evidence to suggest Asians are [disproportionately well-compensated](http://www.pewresearch.org/fact-tank/2016/07/01/racial-gender-wage-gaps-persist-in-u-s-despite-some-progress/) in some professions. Asians are smart. Asians work hard. Smart companies want to hire smart, hard working people to build smart products, and will pay them commensurately. Whether or not Asian bias exists, the results of hiring for diversity has higher variance, on a number of key performance metrics. Since adopting Asian bias, productivity and profits have soared. As a business owner, what would you do?

I have no idea whether the prior assertions are true. But a careful reading will reveal no implied causal relationship between "bias", "productivity" and "profit". The point is, we can replace "bias", "productivity" and "profit" with any variable X, Y and Z. You control X, and observe the effect on Y and Z. Suppose X is positively correlated with Y and Z, i.e. higher values of X correspond to frequently higher values of Y and Z. Lower values correspond with frequently lower Y and Z, with high variance for low values of X. If we want to maximize Z, what is our best strategy?

Bias has and will always exist - machine learning just reflects our biases and makes them more systematic. And bias can be found everywhere, from the smallest neuron in a network to the [largest ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) in a Kaggle competition. As we receive more data, we update the biases. Some grow stronger, and some grow weaker. There will always be more and less accurate biases, depending on what we are measuring, and what outcome we wish to produce. This is why we must choose wisely, our [objective functions](https://en.wikipedia.org/wiki/Loss_function). But even so, AI will only reflect what the data tells it, not the possibilities if the world was a different place.

 Some companies have biases of productivity, intelligence. Some companies But now we can quantify it like never before - and we don't like what it shows. People will condemn the inaccurate biases, and defend the accurate ones. And human decision makers are imperfect statistical estimators, at best. None of this will change. But we need to be careful, because bias, accurate or otherwise, can have long standing effects on a population.

There is a dangerous path Bias in the long term can influence beliefs, influence behavior. Regardless of whether bias is still applicable. Negative and positive effects on a population.

Humans are susceptible to a thousand different irrational biases.

If you are designing AI in a new domain, you should ask yourself three 
important questions to help ensure your model is free from unwanted bias.:

1. Are we really measuring what we want to measure? (Construct validity)
    - Many advertisers try to maximize clicks. This is a [loosing battle](https://en.wikipedia.org/wiki/Click_fraud).
    - Objectives may change over the production lifetime of a model.
    - A poorly chosen objective can have [unintended consequences](https://wiki.lesswrong.com/wiki/Paperclip_maximizer).
2. Is the training data accurate and free from hidden bias? (Internal validity)
    - If the labeling method is biased, the model will encode its bias.
    - The sample population may be skewed for various [reasons](https://en.wikipedia.org/wiki/Internal_validity).
    - Your training data may not be [partitioned](https://en.wikipedia.org/wiki/Test_set) correctly.
3. Is the training data representative of the production data? (External validity)
    - Maybe the true population is not the population we trained for.
    - Maybe the training set is not large enough for valid results.
    - Maybe the model is missing data on some key demographic.

# Work in Progress
<br/>
One of the major limitations of user interfaces is bandwidth - keyboards and screens can only exchange so much information with their users. But today's computers have the ability to interact with their environment in exciting new ways. From self-flying drones to virtual patients, and home appliances to smart assistants, machines are becoming increasingly perceptive, and increasingly conversant. Machines can see, hear, and understand natural language. They can recognize faces and speech, anticipate our intentions and assist with increasingly sophisticated tasks. We call these capabilities "artificial intelligence". But a more apt name might be "[augmented intelligence](https://en.wikipedia.org/wiki/Intelligence_amplification)".

The truth is, the problems we encounter in AI are the same problems we have been struggling with for the greater part of the 20th century. Education.  Egality. Employment. The democratization of technology. Machine learning has many benign use cases, from eye-tracking to handwriting recognition. Things that will make our lives more productive. If you want to contribute to the survival of our species, don't work in redistributing wealth. Work in progress.

### Citations

[ˆ1]: Pavel Chekov
[ˆ2]: Arguably the most famous sci-fi writer in China.
[ˆ3]: https://arxiv.org/abs/1602.04938